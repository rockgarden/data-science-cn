# 数据治理平台

## 开源系统清单

| 系统 | 类型 | 推荐指数 | 适用场景 |
|------|------|----------|----------|
| **Apache Griffin** | 数据质量平台 | ⭐⭐⭐⭐☆ | 大数据质量监控与评估 |
| **Great Expectations** | 数据质量框架 | ⭐⭐⭐⭐⭐ | 规则定义、测试、CI/CD |
| **Apache Hop** | ETL工具&数据集成 | ⭐⭐⭐⭐☆ | 可视化ETL与清洗流程 |
| **Talend Open Studio** | ETL工具&数据集成 | ⭐⭐⭐☆☆ | 复杂转换逻辑（需学习） |
| **DataHub** | 元数据平台 | ⭐⭐⭐⭐☆ | 清洗规则上下文管理 |
| **Apache Atlas** | 元数据平台 | ⭐⭐⭐☆☆ | Hadoop生态元数据管理 |
| **Airflow** | 调度系统 | ⭐⭐⭐⭐⭐ | 清洗任务自动化调度 |
| **DolphinScheduler** | 调度系统 | ⭐⭐⭐⭐☆ | 国产化项目调度 |
| **OpenRefine** | 桌面工具 | ⭐⭐⭐☆☆ | 小数据探索性清洗 |

### 1. **Apache Griffin**

- **官网**：<https://griffin.apache.org/>
- **语言**：Java/Scala
- **特点**：
  - Apache 顶级项目，专为数据质量与数据校验设计
  - 支持**数据质量六大维度**：完整性、准确性、一致性、时效性、唯一性、有效性
  - 提供**数据清洗与校验规则引擎**（基于 DSL）
  - 支持批处理（Spark）和流处理（Streaming）
  - 可定义数据质量度量任务并生成报告
- **适用场景**：
  - 数据入仓前质量校验
  - 清洗效果评估与监控
- **优势**：
  - 与 Spark 深度集成，适合大数据场景
  - 支持可视化仪表盘（需集成前端）
- **不足**：
  - 配置较复杂，学习曲线较高
  - 原生UI较弱，需二次开发

> 推荐作为**数据质量评分 + 清洗效果验证**的核心引擎

### 2. **Great Expectations**

- **官网**：<https://greatexpectations.io/>
- **语言**：Python
- **特点**：
  - 当前最流行的**开源数据质量框架**
  - 支持定义“数据期望”（Expectations），如“字段非空”、“值在范围内”
  - 自动生成数据质量报告（HTML格式）
  - 支持 Pandas、Spark、SQL 数据源
  - 可与 Airflow、Dagster、Prefect 集成
  - 支持**数据清洗建议**（如自动识别缺失值）
- **适用场景**：
  - 数据测试、数据验证、CI/CD in Data
  - 小到中等规模数据清洗规则管理
- **优势**：
  - Python生态友好，易于上手
  - 社区活跃，文档完善
  - 支持 Jupyter Notebook 快速验证
- **不足**：
  - 无原生可视化配置界面（需自研或使用第三方工具）

> 推荐作为**数据质量规则定义 + 清洗前验证**的核心工具

### 3. **Apache Hop（原Pentaho Data Integration）**

- **官网**：<https://hop.apache.org/>
- **语言**：Java
- **特点**：
  - 图形化 ETL 工具，支持拖拽式数据清洗流程设计
  - 内置丰富清洗步骤：去重、格式转换、空值处理、正则替换等
  - 支持元数据管理、数据质量检查
  - 可调度执行（集成 Airflow 或自建调度）
- **优势**：
  - 功能全面，类似商业版 Kettle
  - 支持本地或集群运行
- **适用场景**：
  - 批量数据清洗、数据迁移、数仓构建

> 推荐作为**可视化ETL清洗流程设计工具**

### 4. **Talend Open Studio（开源版）**

- **官网**：<https://www.talend.com/products/talend-open-studio/>
- **语言**：Java
- **特点**：
  - 强大的图形化 ETL 设计器
  - 内置数据质量组件（如地址标准化、数据剖析）
  - 支持生成 Java 代码，可部署到服务器
- **优势**：
  - 功能强大，适合复杂清洗逻辑
  - 支持与 Git 集成，版本管理
- **注意**：
  - 开源版功能有限，高级功能需商业版

> 适合需要**复杂数据转换与清洗逻辑**的团队

### 5. **Apache Atlas**

- **官网**：<https://atlas.apache.org/>
- **语言**：Java
- **特点**：
  - Hadoop 生态元数据管理标准
  - 支持数据分类、标签、血缘分析
  - 可为表/字段打标签（如 `PII`, `phone`, `critical`）
- **与清洗系统集成**：
  - 清洗规则可通过标签（如 `needs_phone_clean`）自动关联到表
- **优势**：
  - 与 Hive、HBase、Spark 集成良好
  - 支持权限与审计
- **不足**：
  - 配置复杂，运维成本高

> 推荐用于**规则与表的标签化关联 + 血缘追踪**

### 6. **DataHub（由 LinkedIn 开源）**

- **官网**：<https://datahubproject.io/>
- **语言**：Python/Java
- **特点**：
  - 现代化元数据平台，支持实时元数据采集
  - 提供强大 UI，支持数据发现、数据血缘、数据质量标签
  - 支持自定义“数据质量断言”（Assertions）
- **优势**：
  - 用户体验优秀，社区活跃
  - 支持与 Great Expectations 集成
  - 可通过 API 管理清洗规则关联
- **适用场景**：
  - 构建企业级数据目录 + 清洗规则上下文管理

> 推荐作为**元数据中枢 + 清洗规则上下文管理平台**

### 7. **Apache Airflow**

- **官网**：<https://airflow.apache.org/>
- **语言**：Python
- **特点**：
  - 最流行的**工作流调度系统**
  - 可将清洗脚本（SQL、Python）封装为 DAG 任务
  - 支持依赖管理、重试、告警、监控
- **与清洗系统集成**：
  - 将 Great Expectations 或 Spark 清洗任务作为 Operator 执行
- **优势**：
  - 成熟稳定，生态丰富
  - 支持动态生成任务

> ⏱️ 推荐作为**清洗任务调度与自动化执行引擎**

### 8. **DolphinScheduler**

- **官网**：<https://dolphinscheduler.apache.org/>
- **语言**：Java
- **特点**：
  - 国产开源调度系统，中文支持好
  - 提供可视化 DAG 设计界面
  - 支持多种任务类型（Shell、SQL、Spark、Python）
- **优势**：
  - 易部署，适合国内团队
  - 社区活跃，文档中文

> 推荐作为 Airflow 的替代方案，尤其适合国企/政企

### 9. **OpenRefine（原Google Refine）**

- **官网**：<https://openrefine.org/>
- **语言**：Java
- **特点**：
  - 桌面级数据清洗工具，适合小数据集（<100万行）
  - 支持数据聚类、格式转换、正则替换、与外部API交互
  - 无需编程，纯图形化操作
- **优势**：
  - 上手极快，适合业务人员
  - 支持导出清洗脚本（JSON格式，可复用）
- **不足**：
  - 不适合大规模数据或自动化

> 推荐作为**探索性数据清洗**或**POC验证工具**

## 推荐组合方案

```txt
数据源 → ETL/Streaming → Great Expectations / Deequ → 质量结果存储（DB）
                             ↓
                      OpenMetadata / DataHub（展示）
                             ↓
                      告警（Prometheus + Alertmanager / Slack）
```

| 场景 | 推荐技术栈组合 |
|------|----------------|
| **企业级数据清洗平台** | `Great Expectations` + `DataHub` + `Airflow` + `Spark` |
| **大数据质量监控** | `Apache Griffin` + `Hive` + `Spark` + `Atlas` |
| **轻量级自动化清洗** | `OpenRefine` → 导出规则 → `Python脚本` + `Airflow` |
| **ETL流程可视化清洗** | `Apache Hop` 或 `Talend Open Studio` |
| **国产化/政企项目** | `DolphinScheduler` + `自研规则引擎` + `DataHub` |

## 业务人员使用

| 目标 | 说明 |
|------|------|
| **无需写代码** | 所有操作通过点击、选择完成 |
| **所见即所得** | 能看到数据、看到规则、看到结果 |
| **质量可视化** | 质量评分、趋势、问题明细一目了然 |
| **自助配置规则** | 可基于模板创建简单清洗/校验规则 |
| **自动告警与反馈** | 问题数据自动通知责任人 |
| **联动清洗任务** | 点击“修复”可触发预设清洗流程 |

1. 用户角色定义

    | 角色 | 权限 |
    |------|------|
    | **业务用户** | 查看表质量、提交问题、查看报告 |
    | **数据运营** | 配置简单规则、查看趋势、触发清洗 |
    | **数据治理员** | 审批规则、管理标签、配置质量阈值 |
    | **系统管理员** | 用户管理、系统配置、集成维护 |

    > 所有角色均通过 **Web 界面操作**，无需接触命令行或代码。

2. 使用流程

    场景：业务人员发现“客户表手机号格式混乱”

    步骤 1：登录 DataHub，进入目标表

    - 路径：`DataHub → 搜索 "客户信息表" → 进入详情页`

    步骤 2：查看数据质量面板（新增标签页）

    - 新增标签页：**“数据质量”**
    - 显示内容：
    - 质量总分：82/100
    - 各维度得分：完整性 ✅、规范性 ❌（低）
    - 最近一次检测时间：2025-04-05 10:00
    - 失败规则：`phone 字段格式不符合 11 位数字`

    > **可视化提示**：规范性打红标，鼠标悬停显示详情

    步骤 3：一键提交“清洗建议”

    - 按钮：`[提交清洗建议]`
    - 弹窗表单：
    - 问题类型：□ 缺失值 □ 格式错误 ☑ 异常值
    - 字段选择：phone
    - 建议操作：标准化手机号格式（自动填充推荐）
    - 关联责任人：自动带出表负责人
    - 提交后，进入“待处理”队列

    步骤 4：数据运营处理建议

    - 数据运营登录后，在“**待办任务**”中看到该建议
    - 点击“应用预设规则”：
    - 选择模板：`通用-手机号格式清洗`
    - 预览效果：显示清洗前后对比（左原始，右清洗后）
    - 点击 `[应用并测试]` → 系统自动运行 GE 验证
    - 点击 `[发布规则]` → 自动同步到 GE 并调度下次执行

    步骤 5：自动触发清洗（后台完成）

    - 下次数据接入时，Airflow 自动执行清洗 + 质量验证
    - 结果自动回写 DataHub，质量分更新为 95

    步骤 6：通知反馈

    - 系统自动邮件通知业务人员：
    > “您反馈的客户表手机号问题已修复，当前数据质量评分为 95 分。”

### 关键功能增强

1. **数据质量概览仪表盘（首页）**

    - 大屏展示：
    - 我负责的表中，质量低于 80 的有 3 张
    - 本周新增质量问题：5 条
    - 清洗任务成功率：98%
    - 支持按业务域、系统、负责人筛选

2. **规则配置向导（Wizard 模式）**

    ```text
    [+] 创建质量规则
    2. 选择表：___________
    3. 选择字段：_________
    4. 选择问题类型：
    ○ 不能为空
    ○ 必须是日期格式
    ○ 必须是 11 位手机号
    ○ 在指定范围内
    5. 设置严重等级：警告 / 失败
    6. [预览规则] → [保存为草稿] / [提交审批]
    ```

    > 所有选项均为下拉或勾选，**无需输入代码或表达式**

3. **清洗模板库（可复用）**

    - 内置常见模板：
    - `客户信息清洗模板`
    - `订单金额合理性校验`
    - `日志时间格式标准化`
    - 用户只需“选择模板 → 关联表 → 启用”，即可自动应用

4. **一键触发临时清洗（沙箱模式）**

    - 按钮：`[临时清洗测试]`
    - 选择规则 → 选择数据范围（如最近 1000 条）
    - 系统在沙箱环境运行，生成报告
    - 结果可导出为 Excel，用于业务验证

5. **质量告警与工单系统集成**

    - 当某表质量分连续 2 天 < 80：
    - 自动打标：`数据质量风险`
    - 自动创建工单（Jira/钉钉/企业微信）
    - 分配给表负责人

### 后台自动化支撑

开发团队维护。

虽然前端对用户“无感”，但后台仍需技术支撑：

| 后台能力 | 说明 |
|----------|------|
| **规则翻译引擎** | 将用户选择的“不能为空” → 转为 GE 的 `expect_column_values_to_not_be_null` |
| **沙箱执行环境** | 隔离测试，避免影响生产 |
| **定期同步机制** | 每日凌晨同步 GE 的最新结果到 DataHub |
| **权限代理** | 用户操作由服务账号代为调用 GE API |

## DataHub 和 OpenMetadata 对比

对非开发人员友好性（User-Friendliness for Non-Developers）** 方面的表现。

详细对比表（面向非开发人员）

| 维度 | **OpenMetadata** | **DataHub** | 胜出方 |
|------|------------------|-------------|--------|
| **用户界面（UI）现代感）** | ✅ 现代、简洁、响应式设计，类似现代 SaaS | ⚠️ 较传统，类似内部系统 | ✅ OpenMetadata |
| **易用性（上手难度）** | ✅ 导航清晰，功能分区明确 | ❌ 学习曲线陡，概念复杂（如“平台”、“实体”） | ✅ OpenMetadata |
| **搜索与发现能力** | ✅ 支持模糊搜索、标签、分类、热度排序 | ✅ 支持搜索，但结果展示较技术化 | ✅ OpenMetadata（更直观） |
| **数据血缘（Lineage）可视化** | ✅ 图形化清晰，可缩放、展开/收起节点 | ✅ 支持，但布局较拥挤，交互略卡 | ✅ OpenMetadata |
| **数据质量结果展示** | ✅ 原生集成 Great Expectations，直接显示校验失败率、趋势图 | ⚠️ 支持，但需自行开发插件或外部集成 | ✅ OpenMetadata |
| **字段级描述与注释** | ✅ 支持富文本、@提人、评论区，类似协作工具 | ✅ 支持，但体验较原始 | ✅ OpenMetadata |
| **报告与可读性** | ✅ 可查看样本数据、质量历史、负责人信息 | ⚠️ 更偏向元数据结构，缺乏“故事性”解读 | ✅ OpenMetadata |
| **中文支持** | ✅ 社区有中文文档和汉化尝试，国内用户多 | ❌ 几乎无中文资源，全英文环境 | ✅ OpenMetadata |
| **文档与学习资源** | ✅ 文档结构清晰，有视频教程 | ⚠️ 文档较技术化，适合开发者 | ✅ OpenMetadata |
| **是否需要代码才能用** | ❌ 不需要，可通过 UI 完成大部分操作 | ⚠️ 很多功能需通过 CLI 或 API 配置 | ✅ OpenMetadata |

## DolphinScheduler

核心理念：DolphinScheduler 是“元数据的生产者”，不是“存储者”

DolphinScheduler 的优势在于：

- 调度大量数据任务（Spark、Hive、Flink、SQL）
- 明确的任务依赖（DAG）
- 记录任务输入输出表（可通过解析 SQL 或配置获取）

因此，**DolphinScheduler 可以成为元数据（尤其是数据血缘）的重要来源**。

1. 如何基于 DolphinScheduler 实现元数据管理？

    我们采用“**以 DolphinScheduler 为驱动，向外部元数据系统上报信息**”的架构：

    ```text
    +---------------------+
    | DolphinScheduler    |
    |  - DAG 任务流        |
    |  - SQL 脚本          |
    |  - 执行日志          |
    +----------+----------+
              | 提取元数据
              v
    +---------------------+
    | 元数据采集脚本       | —→ 解析任务、提取表依赖
    +----------+----------+
              | 上报
              v
    +---------------------+
    | 元数据系统           | —→ OpenMetadata / DataHub / Atlas
    |  - 数据目录          |
    |  - 血缘图谱          |
    |  - 搜索与影响分析     |
    +---------------------+
    ```

    关键元数据类型与提取方式

    | 元数据类型 | 如何从 DolphinScheduler 获取 | 提取方法 |
    |-----------|-------------------------------|--------|
    | 🔹 任务依赖（DAG） | DolphinScheduler 原生支持任务前后置依赖 | API 获取 workflow 和 task 关系 |
    | 🔹 输入/输出表 | 从 SQL 脚本中解析 `SELECT a FROM A JOIN B` | SQL 解析器（如 sqllineage） |
    | 🔹 任务负责人 | DolphinScheduler 支持任务级“告警组”和“负责人” | API 获取 task owner |
    | 🔹 执行周期 | 任务调度周期（如每天 00:00） | 从 workflow 定时设置获取 |
    | 🔹 数据质量结果 | 结合 GE/Soda 校验任务结果 | 任务输出日志或 API 获取 |

2. 实现数据血缘自动上报

    步骤 1：准备 DolphinScheduler API 访问

    DolphinScheduler 提供 REST API，用于获取项目、工作流、任务信息。

    ```bash
    # 获取工作流实例
    GET http://ds-api:12345/dolphinscheduler/projects/data_warehouse/workflows/dwd_etl

    # 获取任务实例
    GET http://ds-api:12345/dolphinscheduler/projects/data_warehouse/tasks?workflowId=100
    ```

    文档：<https://dolphinscheduler.apache.org/zh-cn/docs/latest/user_doc/guide/api/api-overview.html>

    步骤 2：解析 SQL 获取输入输出表

    使用 Python 工具 **`sqllineage`** 自动解析 SQL 血缘。

    ```bash
    pip install sqllineage
    ```

    ```python
    from sqllineage.runner import LineageRunner

    sql = """
    INSERT INTO hive.dwd_orders
    SELECT a.order_id, b.user_name
    FROM mysql.source_orders a
    JOIN hive.dim_users b ON a.user_id = b.user_id
    """

    result = LineageRunner(sql)
    print("Inputs:", result.source_tables)   # [mysql.source_orders, hive.dim_users]
    print("Output:", result.target_tables)   # [hive.dwd_orders]
    ```

    步骤 3：构建血缘并上报 OpenMetadata

    ```python
    import requests

    def create_lineage_in_openmetadata(from_table, to_table, pipeline="dolphinscheduler-etl"):
        """
        向 OpenMetadata 上报血缘关系
        """
        url = "http://openmeta8585/api/v1/lineage"
        payload = {
            "edge": {
                "fromEntity": {"id": get_entity_id(from_table), "type": "table"},
                "toEntity": {"id": get_entity_id(to_table), "type": "table"}
            },
            "description": f"Generated from DolphinScheduler pipeline: {pipeline}"
        }
        requests.post(url, json=payload, headers={"Content-Type": "application/json"})
    ```

    > `get_entity_id()` 需先通过 OpenMetadata API 查询表的唯一 ID

    步骤 4：将脚本集成到 DolphinScheduler 工作流

    在关键 ETL 任务后添加一个“**上报元数据**”任务：

    ```text
    [Task 1: 执行 Hive SQL] 
        ↓
    [Task 2: Python 脚本] → 调用 sync_metadata.py 提取并上报血缘
        ↓
    [Task 3: 数据质量校验]
    ```

    ✅ 这样就能实现“任务一完成，血缘自动更新”。

3. 增强功能建议

    | 功能 | 实现方式 |
    |------|----------|
    | 🌐 自动化血缘 | 定期运行脚本，扫描所有 DAG 并更新血缘 |
    | 🔍 字段级血缘 | 使用更高级解析器（如 `sqlglot`）分析列映射 |
    | 📊 元数据门户 | 在 OpenMetadata 中展示“来自 DolphinScheduler 的任务” |
    | 🔔 血缘变更告警 | 当新增表依赖时，发送企业微信/钉钉通知 |
    | 📅 版本管理 | 将元数据快照存入数据库，支持回溯 |

4. 与主流元数据系统集成方式

    | 目标系统 | 集成方式 |
    |--------|----------|
    | **OpenMetadata** | 使用 `/api/v1/lineage` API 上报血缘（推荐） |
    | **DataHub** | 使用 `datahub` CLI 或 REST API 发送 MCP（Metadata Change Proposal） |
    | **Apache Atlas** | 使用 `/api/atlas/v2/entity/bulk` 创建关系 |
    | **自研系统** | 将元数据写入 MySQL/Elasticsearch，构建内部数据地图 |

5. DolphinScheduler 元数据管理的局限性

    | 限制 | 说明 |
    |------|------|
    | ❌ 不支持字段级血缘 | 仅能通过 SQL 解析间接获取 |
    | ❌ 无内置数据目录 | 无法搜索“哪些任务用了 user_id 字段” |
    | ❌ 无数据质量展示 | 需结合 GE/Soda 才能显示校验结果 |
    | ❌ 无数据预览 | 不能直接查看表内容 |

    👉 所以 **DolphinScheduler 不能单独作为元数据系统**，必须与 OpenMetadata 等配合使用。

6. 推荐架构：DolphinScheduler + OpenMetadata

```text
+---------------------+
|  数据源               |
|  MySQL / Hive / Kafka |
+----------+----------+
           |
           v
+---------------------+
| DolphinScheduler     | —— 调度 ETL 任务
|  - DAG 编排           |
|  - 任务依赖           |
+----------+----------+
           | 提取任务信息 + SQL
           v
+---------------------+
| 元数据同步服务         | —— 定时或事件触发
|  - 解析 SQL 血缘       |
|  - 上报 OpenMetadata   |
+----------+----------+
           |
           v
+---------------------+
| OpenMetadata        | —— 元数据中枢
|  - 数据目录           |
|  - 血缘图谱           |
|  - 质量展示           |
|  - 搜索与影响分析      |
+---------------------+
           |
           v
+---------------------+
| 业务用户               |
| 查看“我的表从哪来？”    |
+---------------------+
```
