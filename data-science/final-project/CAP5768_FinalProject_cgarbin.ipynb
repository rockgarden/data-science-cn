{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p78j872pOKTZ"
   },
   "source": [
    "# CAP 5768 - Data Science - Dr. Marques - Fall 2019\n",
    "\n",
    "Christian Garbin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYmNMHGLOKTa"
   },
   "source": [
    "## FINAL PROJECT\n",
    "## Starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UMnsRndOKTb"
   },
   "source": [
    "### Goals \n",
    "\n",
    "- To learn how to implement a Data Science / Machine Learning workflow in Python (using Pandas, Scikit-learn, Matplotlib, and Numpy)\n",
    "- To get acquainted with representative datasets and problems in data science and machine learning\n",
    "- To learn how to implement several different machine learning models in Python \n",
    "- To learn how to evaluate and fine-tune the performance of a model using cross-validation\n",
    "- To learn how to test a model and produce a set of plots and performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0rEAT7MOKTb"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "- This assignment is structured in 3 parts.\n",
    "- As usual, there will be some Python code to be written and questions to be answered.\n",
    "- At the end, you should export your notebook to PDF format; it will become your report.\n",
    "- Submit the report (PDF), notebook (.ipynb file), and (optionally) link to the \"live\" version of your solution on Google Colaboratory via Canvas.\n",
    "- The total number of points is 195 (plus up to 100 bonus points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qiufouQn6OD9"
   },
   "source": [
    "### Important\n",
    "\n",
    "- For the sake of reproducibility, use `random_state=0` (or equivalent) in all functions that use random number generation.\n",
    "- It is OK to attempt the bonus points, but please **do not overdo it!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4dkSabDG5gjc",
    "outputId": "3c3cdd00-171e-4e91-9bcc-67f79eceeb70"
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## Part 1: Decision trees\n",
    "\n",
    "In this part, we will take another look at the Iris dataset.\n",
    "\n",
    "The Python code below will load a dataset containing information about three types of Iris flowers that had the size of its petals and sepals carefully measured.\n",
    "\n",
    "The Fisher’s Iris dataset contains 150 observations with 4 features each: \n",
    "- sepal length in cm; \n",
    "- sepal width in cm; \n",
    "- petal length in cm; and \n",
    "- petal width in cm. \n",
    "\n",
    "The class for each instance is stored in a separate column called “species”. In this case, the first 50 instances belong to class Setosa, the following 50 belong to class Versicolor and the last 50 belong to class Virginica.\n",
    "\n",
    "See:\n",
    "https://archive.ics.uci.edu/ml/datasets/Iris for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4dkSabDG5gjc",
    "outputId": "3c3cdd00-171e-4e91-9bcc-67f79eceeb70"
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Your turn! (25 points)\n",
    "\n",
    "Write code to: \n",
    "\n",
    "1. Display the pair plots for all (4) attributes for all (3) categories/species/classes in the Iris dataset. (15 pts)\n",
    "2. Compute relevant summary statistics for each species. (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOZPjis7Phf"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQkN1WvQOKTi"
   },
   "source": [
    "### 1. Display the pair plots for all (4) attributes for all (3) categories/species/classes in the Iris dataset. (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute relevant summary statistics for each species. (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using DataFrame `describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species in iris.species.unique():\n",
    "    print(\"Summary statistics for {}\".format(species))\n",
    "    display(iris[iris.species == species].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the package [`ydata-profiling`](https://ydata-profiling.ydata.ai/docs/master/index.html).\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. Even on the small Iris dataset it takes several seconds to run. Larger dataset may take a minute or more to complete. See [their documentation](https://ydata-profiling.ydata.ai/docs/master/pages/use_cases/big_data.html) for hints to work with large datasets.\n",
    "1. It produces an HTML report with tabs. It's not useful when exporting a notebook to formats that don't support HTML rendering (e.g. PDF). But we can export the report directly to a file. Look for \"export\" in the [quick start guide](https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(iris, title=\"Iris Profiling Report\", explorative=True)\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Your turn! (35 points)\n",
    "\n",
    "Write code to: \n",
    "\n",
    "1. Build a decision tree classifier using scikit-learn's `DecisionTreeClassifier` (using the default options). Check documentation at https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html (10 pts)\n",
    "2. Plot the resulting decision tree. It should look similar to the plot below. (15 pts)\n",
    "(Note: if `graphviz` gives you headaches, a text-based 'plot'-- using `export_text` -- should be OK.)\n",
    "3. Perform k-fold cross-validation using k=3 and display the results. (10 pts)\n",
    "\n",
    "![decision tree](notebook-images/decision-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOZPjis7Phf"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQkN1WvQOKTi"
   },
   "source": [
    "### 1. Build a decision tree classifier using scikit-learn's `DecisionTreeClassifier` (using the default options). Check documentation at https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features: measurements\n",
    "X = iris.iloc[:, :-1]\n",
    "# The label: species\n",
    "y = iris.species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the decision tree with default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier(random_state=0)\n",
    "dtc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot the resulting decision tree. It should look similar to the plot below. (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot requires [Graphviz](https://www.graphviz.org/). The code is based on [this article](https://scikit-learn.org/stable/modules/tree.html#tree). It exports the decision tree to a PNG file because displaying it directly on the notebooks uses SVG. Exporting a notebook that has an SGV picture to PDF is a major pain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(\n",
    "    dtc,\n",
    "    out_file=None,\n",
    "    feature_names=X.columns,\n",
    "    class_names=y.unique(),\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    ")\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(filename=\"iris-decision-tree\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Iris decision tree](iris-decision-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform k-fold cross-validation using k=3 and display the results. (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we need to specify a k-fold cross-validator because the default for this case is a _stratified_ k-fold_ ([`cross_val_score` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)):\n",
    "\n",
    "> cv : int, cross-validation generator or an iterable, optional\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, `StratifiedKFold` is used. In all other cases, `KFold` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Shuffling is a must here because the dataset may be ordered\n",
    "# (the folds would contain only some classes in that case)\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "cross_val_score(dtc, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sf9JB_ntOKTg"
   },
   "source": [
    "## Bonus opportunity 1 (15 points)\n",
    "\n",
    "Make meaningful changes to the baseline code, e.g., trying different combinations of functions to measure the quality of a split, limiting the maximum depth of the tree, etc. \n",
    "\n",
    "Publish the code, the results, and comment on how they differ from the baseline (and your intuition as to *why* they do)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOZPjis7Phf"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will try some parameters that affect the tree creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQkN1WvQOKTi"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# First  value is the deafault value\n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "    \"max_depth\": [None, 3, 4, 5],\n",
    "}\n",
    "\n",
    "# Use `verbose` to track progress - this may take several minutes\n",
    "gs = GridSearchCV(\n",
    "    tree.DecisionTreeClassifier(random_state=0), param_grid, cv=5, n_jobs=-1, verbose=3\n",
    ")\n",
    "\n",
    "gsc = gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters found with grid search:\")\n",
    "print(gsc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling is a must here because the dataset may be ordered\n",
    "# (the folds would contain only some classes in that case)\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "cross_val_score(gsc.best_estimator_, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: grid searched resulted in the default values for `criterion` and `max_depth`, and a new value for `splitter`. However, the final scores were on average worse than the default values. This indicates the dataset is relatively simple (it is indeed), so not much fine tuning is needed to make a decision tree perform well on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## Part 2: Digit classification\n",
    "\n",
    "The MNIST handwritten digit dataset consists of a training set of 60,000 examples, and a test set of 10,000 examples. Each image in the dataset has 28 $\\times$ 28 pixels. They are saved in the csv data files `mnist_train.csv` and `mnist_test.csv`. \n",
    "\n",
    "Every line of these files consists of a grayscale image and its label, i.e. 785 numbers between 0 and 255:\n",
    "- The first number of each line is the label, i.e. the digit which is depicted in the image. \n",
    "- The following 784 numbers are the pixels of the 28 $\\times$ 28 image.\n",
    "\n",
    "The Python code below loads the images from CSV files, normalizes them (i.e., maps the intensity values from [0..255] to [0..1]), and displays a few images from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28  # width and length\n",
    "no_of_different_labels = 10  #  i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size\n",
    "data_path = \"data/\"\n",
    "train_data = np.loadtxt(data_path + \"mnist_train.csv.gz\", delimiter=\",\")\n",
    "test_data = np.loadtxt(data_path + \"mnist_test.csv.gz\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.asfarray(train_data[:, 1:]) / 255.0\n",
    "test_imgs = np.asfarray(test_data[:, 1:]) / 255.0\n",
    "train_labels = np.asfarray(train_data[:, :1])\n",
    "test_labels = np.asfarray(test_data[:, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 4, figsize=(2, 2))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(train_imgs[i].reshape((28, 28)), cmap=\"Greys\")\n",
    "    axi.set(xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Your turn! (20 points)\n",
    "\n",
    "Write code to: \n",
    "\n",
    "1. Build and fit a 10-class Naive Bayes classifier using scikit-learn's `MultinomialNB()` with default options and using the raw pixel values as features. (5 pts)\n",
    "2. Make predictions on the test data, compute the overall accuracy and plot the resulting confusing matrix. (15 pts)\n",
    "\n",
    "Hint: your accuracy will be around 83.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOZPjis7Phf"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data: `fit()` and `predict()` expect a 1D array. However, the labels are stored in a multi-dimensional array. Here we will reshape them into a 1D array. This could be done inline, when calling `fit()` and `reshape()`. It's done here for clarity and to document the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels before reshaping\")\n",
    "print(train_labels.shape)\n",
    "print(train_labels[:3])\n",
    "\n",
    "train_labels_1d = train_labels.ravel()\n",
    "test_labels_1d = test_labels.ravel()\n",
    "\n",
    "print(\"\\nLabels after reshaping\")\n",
    "print(train_labels_1d.shape)\n",
    "print(train_labels_1d[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build and fit a 10-class Naive Bayes classifier using scikit-learn's MultinomialNB() with default options and using the raw pixel values as features. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQkN1WvQOKTi"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nbc = MultinomialNB()\n",
    "nbc.fit(train_imgs, train_labels_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predictions on the test data, compute the overall accuracy and plot the resulting confusing matrix. (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def evaluate_classifier(clf):\n",
    "    print(\"Classifier: {}\".format(clf.__class__.__name__))\n",
    "\n",
    "    pred = clf.predict(test_imgs)\n",
    "\n",
    "    print(\"\\nAccuracy: {}\".format(accuracy_score(test_labels_1d, pred)))\n",
    "\n",
    "    print(\"\\nConfusion matrix (text):\")\n",
    "    cm = confusion_matrix(test_labels_1d, pred)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\nConfusion matrix (heatmap) - mistakes only:\")\n",
    "    # Remove correct predictions to make mistakes stand out\n",
    "    np.fill_diagonal(cm, 0)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False, cmap=\"Blues\")\n",
    "    ax.set_ylabel(\"Actual digit\")\n",
    "    ax.set_xlabel(\"Predicted digit\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier(nbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Your turn! (20 points)\n",
    "\n",
    "Write code to: \n",
    "\n",
    "1. Build and fit a 10-class Random Forests classifier using scikit-learn's `RandomForestClassifier()` with default options (don't forget `random_state=0`) and using the raw pixel values as features. (5 pts)\n",
    "2. Make predictions on the test data, compute the overall accuracy and plot the resulting confusing matrix. (15 pts)\n",
    "\n",
    "Hint: your accuracy should be > 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOZPjis7Phf"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQkN1WvQOKTi"
   },
   "source": [
    "### 1. Build and fit a 10-class Random Forests classifier using scikit-learn's `RandomForestClassifier()` with default options (don't forget `random_state=0`) and using the raw pixel values as features. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# n_jobs=-1 speeds it up by about 3x on my computer\n",
    "# n_estimator=10 to avoid future warning\n",
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=10, random_state=0)\n",
    "rfc.fit(train_imgs, train_labels_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predictions on the test data, compute the overall accuracy and plot the resulting confusing matrix. (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Your turn! (20 points)\n",
    "\n",
    "Write code to: \n",
    "\n",
    "1. Build and fit a 10-class classifier of your choice, with sensible initialization options, and using the raw pixel values as features. (5 pts)\n",
    "2. Make predictions on the test data, compute the overall accuracy and plot the resulting confusing matrix. (15 pts)\n",
    "\n",
    "Hint: A variation of the Random Forests classifier from 2.2 above is acceptable. In that case, document your selection of (hyper)parameters and your rationale for choosing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOZPjis7Phf"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build and fit a 10-class classifier of your choice, with sensible initialization options, and using the raw pixel values as features. (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will use grid search to find a better `RandomForestClassifer`.\n",
    "\n",
    "The choice of the classifier was driven by:\n",
    "\n",
    "1. Being able to compare with `RandomForestClassifier` with default values used in the section above, i.e. how much better can it get if we spend time fine-turning it.\n",
    "1. Concentrating more on the process of choosing a classifier, than the classifier itself. More specifically, to spend more time learning how to use `GridSearchCV` and analyze its results, to apply it in the future with other classifier.\n",
    "\n",
    "Parameters to try (and to not try):\n",
    "\n",
    "* `n_estimators`: the default value of 10, and larger values, to create larger ensembles. The premise is that more trese result in better accuracy.\n",
    "* `bootstrap`: the default value of `True` and `False` (motivated by [this discussion in Stack Overflow](https://stats.stackexchange.com/questions/354336/what-happens-when-bootstrapping-isnt-used-in-sklearn-randomforestclassifier)).\n",
    "* `min_samples_split` and `min_sample_leaf`: _not_ used because at first I thought we should try larger values of these two parameters to reduce overfitting for large values of `n_estimators`, but the fact that we have an ensemble already reduces overfitting: \"_...uses averaging to improve the predictive accuracy and control over-fitting_\" ([scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)).\n",
    "* `criterion`: _not_ used because \"[s]tudies have shown that the choice of impurity measure has little effect on the peform of decision tree induction algorithms.\" ([source](https://stats.stackexchange.com/questions/19639/which-is-a-better-cost-function-for-a-random-forest-tree-gini-index-or-entropy), and also [this blog post](https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria), where the first source came from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQkN1WvQOKTi"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 100, 200, 300],\n",
    "    \"bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "# Use `verbose` to track progress - this may take several minutes\n",
    "# Use cv=3 as a compromise between lower runtime and good validation\n",
    "# This may take a few minutes to complete...\n",
    "gs = GridSearchCV(\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=0), param_grid, cv=3, verbose=3\n",
    ")\n",
    "\n",
    "gsc = gs.fit(train_imgs, train_labels_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQkN1WvQOKTi"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters found with grid search:\")\n",
    "print(gsc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predictions on the test data, compute the overall accuracy and plot the resulting confusing matrix. (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier(gsc.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the details of all classifiers tried in the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gsc.cv_results_)\n",
    "# Use only the columns we are intersted int\n",
    "df = df[[\"rank_test_score\", \"mean_test_score\", \"param_bootstrap\", \"param_n_estimators\"]]\n",
    "df.set_index(\"rank_test_score\", inplace=True)\n",
    "display(df.sort_values(by=\"rank_test_score\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the ensemble with 200 and the one with 300 estimators is insignificant. For most applications, the smaller ensemble, with 200 estimators, would be better (use less memory and estimate faster). Depending on how accuracy the application needs, even the ensemble with 100 estimators would be a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## Part 3: Face Recognition \n",
    "\n",
    "In this part you will build a face recognition solution.\n",
    "\n",
    "We will use a subset of the Labeled Faces in the Wild (LFW) people dataset: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html\n",
    "\n",
    "The Python code below loads a dataset of 1867 images (resized to 62 $\\times$ 47 pixels) from the dataset and displays some of them.\n",
    "\n",
    "Hint: you will have to install Pillow for this part. See https://pillow.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "faces = fetch_lfw_people(min_faces_per_person=40)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 15, 15\n",
    "fig, ax = plt.subplots(3, 5, figsize=(8, 4))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap=\"bone\")\n",
    "    axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Your turn! (55 points)\n",
    "\n",
    "Write code to: \n",
    "\n",
    "1. Use Principal Component Analysis (PCA) to reduce the dimensionality of each face to the first 120 components. (10 pts)\n",
    "2. Build and fit a multi-class SVM classifier, with sensible initialization options, and using the PCA-reduced  features. (10 pts)\n",
    "3. Make predictions on the test data, compute the precision, recall and f1 score for each category, compute the overall accuracy, and plot the resulting confusing matrix. (25 pts)\n",
    "4. Display examples of correct and incorrect predictions (at least 5 of each). (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOZPjis7Phf"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: based on the [support vector machine examples from the Python Data Science Handbook](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb#scrollTo=ta09ltcksXnR.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into a training and a testing set before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    faces.data, faces.target, stratify=faces.target, test_size=0.25, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the classes to see if we have training and testing sets that are (roughly) equally distributed. We expect to see bars that are roughly the same relative height. The actual values are not important, but the relative values are. This indicates the training and testing sets have similar distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "_, ax = plt.subplots(1, 3, figsize=(10, 4))\n",
    "sns.countplot(x=faces.target, ax=ax[0])\n",
    "sns.countplot(x=ytrain, ax=ax[1])\n",
    "sns.countplot(x=ytest, ax=ax[2])\n",
    "\n",
    "# Turn of x labels\n",
    "ax[0].set_xticklabels([])\n",
    "ax[1].set_xticklabels([])\n",
    "ax[2].set_xticklabels([])\n",
    "\n",
    "# Add titles\n",
    "ax[0].set_title(\"Target\")\n",
    "ax[1].set_title(\"Training set\")\n",
    "ax[2].set_title(\"Test set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high class imbalance shown in the graph also explains why we needed to use `stratify` in the split and why we will use `class_weight='balanced'` later in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQkN1WvQOKTi"
   },
   "source": [
    "### 1. Use Principal Component Analysis (PCA) to reduce the dimensionality of each face to the first 120 components. (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although  [Python Data Science Handbook](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb#scrollTo=kOGVbQZWn4J-) use the old `RandomizedPCA`(today's `svd_solver='randomized'`), using a randomized SVC resulted in a lower accuracy, so it was removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use `whiten=True` ([source](http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/)):\n",
    "\n",
    "> If we are training on images, the raw input is redundant, since adjacent pixel values are highly correlated. The goal of whitening is to make the input less redundant; more formally, our desiderata are that our learning algorithms sees a training input where (i) the features are less correlated with each other, and (ii) the features all have the same variance.\n",
    "\n",
    "This parameter was crucial to get over 70% accuracy. When set to `False`, accuracy was below 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "NUM_COMPONENTS = 120\n",
    "pca = PCA(n_components=NUM_COMPONENTS, whiten=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build and fit a multi-class SVM classifier, with sensible initialization options, and using the PCA-reduced features. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svc = SVC(kernel=\"rbf\", class_weight=\"balanced\", random_state=0)\n",
    "model = make_pipeline(pca, svc)\n",
    "\n",
    "param_grid = {\n",
    "    \"svc__C\": [1, 5, 10, 50],\n",
    "    \"svc__gamma\": [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "}\n",
    "\n",
    "# Use `verbose` to track progress - this may take several minutes\n",
    "# Choice of parameters:\n",
    "#   cv=3: silence future warning and keep it to a reasonable value\n",
    "#   n_jobs=-1: run as many searches in parallel as possible\n",
    "# This may take a few minutes to complete...\n",
    "grid = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=3)\n",
    "grid.fit(Xtrain, ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make predictions on the test data, compute the precision, recall and f1 score for each category, compute the overall accuracy, and plot the resulting confusing matrix. (25 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcclf = grid.best_estimator_\n",
    "pred = svcclf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAccuracy: {}\".format(accuracy_score(ytest, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_faces_cm(pred):\n",
    "    print(\"\\nConfusion matrix (heatmap) - mistakes only:\")\n",
    "\n",
    "    cm = confusion_matrix(ytest, pred)\n",
    "    # Remove correct predictions to make mistakes stand out\n",
    "    np.fill_diagonal(cm, 0)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cbar=False,\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=faces.target_names,\n",
    "        yticklabels=faces.target_names,\n",
    "    )\n",
    "    plt.xlabel(\"predicted label\")\n",
    "    plt.ylabel(\"true label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_faces_cm(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(ytest, pred, target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Display examples of correct and incorrect predictions (at least 5 of each). (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary function to plot faces.\n",
    "\n",
    "Note: uses some global variables. In the production code should they should be parameters for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_faces(faces_to_show, labels_to_show, num_faces, show_name=True):\n",
    "    # `squeeze=False` so `ax.flat` works with only one picture\n",
    "    fig, ax = plt.subplots(1, num_faces, figsize=(num_faces, 1.5), squeeze=False)\n",
    "\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        axi.imshow(faces_to_show[i].reshape(62, 47), cmap=\"bone\")\n",
    "        axi.set(xticks=[], yticks=[])\n",
    "        if show_name:\n",
    "            axi.set_xlabel(faces.target_names[labels_to_show[i]].split()[-1][:10])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show correct and incorrrect predictions, one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(faces.target_names):\n",
    "    print(\"\\n{} --------------\".format(name))\n",
    "\n",
    "    # Correct predictions\n",
    "    mask = (ytest == i) & (pred == i)\n",
    "    sum_faces = sum(mask)\n",
    "    num_faces = min(10, sum(mask))\n",
    "    print(\"{} (of {}) correct predictions\".format(num_faces, sum_faces))\n",
    "    plot_faces(Xtest[mask], pred[mask], num_faces, show_name=False)\n",
    "\n",
    "    # False negative: predicted as someone else\n",
    "    mask = (ytest == i) & (pred != i)\n",
    "    sum_faces = sum(mask)\n",
    "    num_faces = min(10, sum(mask))\n",
    "    print(\"{} (of {}) {} predicted as someone else\".format(num_faces, sum_faces, name))\n",
    "    if num_faces > 0:\n",
    "        plot_faces(Xtest[mask], pred[mask], num_faces)\n",
    "\n",
    "    # False positive: someone else predicted as this person\n",
    "    mask = (ytest != i) & (pred == i)\n",
    "    sum_faces = sum(mask)\n",
    "    num_faces = min(10, sum(mask))\n",
    "    print(\"{} (of {}) someone else predicted as {}\".format(num_faces, sum_faces, name))\n",
    "    if num_faces > 0:\n",
    "        plot_faces(Xtest[mask], ytest[mask], num_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sf9JB_ntOKTg"
   },
   "source": [
    "## Bonus opportunity 2 (35 points)\n",
    "\n",
    "Make meaningful changes to the baseline code, e.g.:\n",
    "\n",
    "- trying different combinations of SVM parameters following a grid search cross-validation approach.\n",
    "- experimenting with different values of number of components for the PCA and showing how much of the variance they explain (i.e., plotting the cumulative explained variance as a function of the number of components).\n",
    "- using \"data augmentation\" to generate additional training images (for under-represented classes).\n",
    "\n",
    "Publish the code, the results, and document your steps and the rationale behind them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOZPjis7Phf"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQkN1WvQOKTi"
   },
   "source": [
    "### trying different combinations of SVM parameters following a grid search cross-validation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was done in [part 2 above](#2.-Build-and-fit-a-multi-class-SVM-classifier,-with-sensible-initialization-options,-and-using-the-PCA-reduced-features.-(10-pts))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experimenting with different values of number of components for the PCA and showing how much of the variance they explain (i.e., plotting the cumulative explained variance as a function of the number of components)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: create a PCA with the maximum number of components, to inspect variabilit of the entire range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of components that PCA accepts\n",
    "n_components = min(len(faces.data), len(faces.data[0]))\n",
    "print(\"Total number of components: {}\".format(n_components))\n",
    "\n",
    "pca_var = PCA(n_components=n_components, whiten=True, random_state=0)\n",
    "pca_var.fit(faces.data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: calculate and plot the cumulative variance (as a function of the number of components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_var = np.cumsum(pca_var.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cum_var)\n",
    "plt.xlabel(\"number of components\")\n",
    "plt.ylabel(\"cumulative explained variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: calculate the number of components needed to explain several levels of variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for percent in [0.1, 0.25, 0.5, 0.75, 0.9, 0.99]:\n",
    "    break_point = cum_var[-1] * percent\n",
    "    n_components_pct = np.argmax(cum_var >= break_point)\n",
    "    print(\n",
    "        \"Number of components for {:.0%} variance:\"\n",
    "        \" {:3d} (actual percentage: {:.5%})\".format(\n",
    "            percent, n_components_pct, cum_var[n_components_pct]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: 513 components are able to explain 99% of variability in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How one of the images looks like when using the number of components that explains 99% of variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pic = PCA(n_components=442, whiten=True, random_state=0)\n",
    "pca_pic.fit(faces.data)\n",
    "\n",
    "components = pca_pic.transform(faces.data)\n",
    "projected = pca_pic.inverse_transform(components)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(4, 4), subplot_kw={\"xticks\": [], \"yticks\": []})\n",
    "ax[0].imshow(faces.data[0].reshape(62, 47), cmap=\"bone\")\n",
    "ax[1].imshow(projected[0].reshape(62, 47), cmap=\"bone\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using \"data augmentation\" to generate additional training images (for under-represented classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "    \n",
    "* [Thomas Himblot's Medium article](https://medium.com/@thimblot/data-augmentation-boost-your-image-dataset-with-few-lines-of-python-155c2dc1baec)\n",
    "* [Conner Shorten's Towards Data Science article](https://towardsdatascience.com/image-augmentation-examples-in-python-d552c26f2873)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of images we have to add for each person to match the person with the most number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_person = np.bincount(ytrain)\n",
    "max_images = max(num_images_person)\n",
    "num_missing_images = max_images - num_images_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An auxiliary function to augment images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage as sk\n",
    "from skimage import transform\n",
    "\n",
    "# To make it consistent across runs\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def add_images(person, num_images_to_add, X, y, show_changes=True):\n",
    "    \"\"\"Augment images for the given person. Images are appended to the existing\n",
    "    image array. Label array is also extended to match the images array.\"\"\"\n",
    "    if num_images_to_add == 0:\n",
    "        return X, y\n",
    "\n",
    "    existing_images = Xtrain[ytrain == person]\n",
    "    num_existing_images = len(existing_images)\n",
    "\n",
    "    # The original images, before manipulating\n",
    "    old_images = []\n",
    "    # Images after manipulation\n",
    "    new_images = []\n",
    "    # The changes applied to the image\n",
    "    changes = []\n",
    "\n",
    "    for _ in range(num_images_to_add):\n",
    "        # Pick one of the existing images at random (with substitution)\n",
    "        image = existing_images[np.random.randint(0, num_existing_images)]\n",
    "        old_images.append(image)\n",
    "        image = image.reshape(62, 47)\n",
    "\n",
    "        # Rotate right or left by a random amount\n",
    "        # Range of angles found empirically\n",
    "        random_degree = np.random.randint(-10, 10)\n",
    "        image = transform.rotate(image, random_degree)\n",
    "        change = \"{}\".format(random_degree)\n",
    "\n",
    "        # Flip some of the images horizontally\n",
    "        if np.random.randint(0, 100) <= 25:\n",
    "            image = np.fliplr(image)\n",
    "            change = \"{},f\".format(change)\n",
    "\n",
    "        # Intensity (similar to brightness/contrast)\n",
    "        # Code from https://stackoverflow.com/a/19384041/336802\n",
    "        phi, theta = 1, 1  # need to play with these values\n",
    "        max_intensity = 255.0\n",
    "        if np.random.randint(0, 100) <= 10:\n",
    "            # Increase intensity\n",
    "            image = (max_intensity / phi) * (image / (max_intensity / theta)) ** 0.5\n",
    "            change = \"{},i\".format(change)\n",
    "        elif np.random.randint(0, 100) <= 10:\n",
    "            # Decrease intensity\n",
    "            image = (max_intensity / phi) * (image / (max_intensity / theta)) ** 2\n",
    "            change = \"{},d\".format(change)\n",
    "\n",
    "        new_images.append(image.ravel())\n",
    "        changes.append(change)\n",
    "\n",
    "    new_labels = [person] * num_images_to_add\n",
    "\n",
    "    if show_changes:\n",
    "        N = 8\n",
    "        print(\"Before changes ({} samples):\".format(N))\n",
    "        plot_faces(old_images[:N], new_labels[:N], N, False)\n",
    "        print(\"After changes:\")\n",
    "        plot_faces(new_images[:N], new_labels[:N], N, False)\n",
    "        print(changes[:8])\n",
    "\n",
    "    X = np.append(X, np.array(new_images), axis=0)\n",
    "    y = np.append(y, np.array(new_labels))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new images for each person. After this is done, all persons in the dataset will have the same number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xtrain\n",
    "y = ytrain\n",
    "\n",
    "for i, name in enumerate(faces.target_names):\n",
    "    print(\"\\nAdding {:3d} images for {}\".format(num_missing_images[i], name))\n",
    "    X, y = add_images(i, num_missing_images[i], X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we indeed added the number images we meant to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.bincount(y) == max_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the dataset to ensure we don't have large seqeunce of images for the same person. Some machine learning algorithms are sensitive to the order of the samples. Althought we don't need to shuffle for random forests, this prevents silly mistakes in the future, if we try other solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new support vector classifer with the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=NUM_COMPONENTS, whiten=True, random_state=0)\n",
    "svc = SVC(kernel=\"rbf\", class_weight=\"balanced\", random_state=0)\n",
    "model = make_pipeline(pca, svc)\n",
    "\n",
    "# This range of values was tweaked to work with the augmented dataset\n",
    "param_grid = {\"svc__C\": [1, 5, 10], \"svc__gamma\": [0.001, 0.01, 0.05]}\n",
    "\n",
    "# Use `verbose` to track progress - this may take several minutes\n",
    "# Choice of parameters:\n",
    "#   cv=3: silence future warning and keep it to a reasonable value\n",
    "# This may take a few minutes to complete...\n",
    "grid_aug = GridSearchCV(model, param_grid, cv=3, verbose=3)\n",
    "grid_aug.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_aug.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classifier with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcclf_aug = grid_aug.best_estimator_\n",
    "pred_aug = svcclf_aug.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAccuracy: {}\".format(accuracy_score(ytest, pred_aug)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_faces_cm(pred_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, pred_aug, target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of this exercise: it did not improve the results. A possible reason for this result is that the image augmentation code is rather simple. It could be enhanced with more sophisticated image augmentation techniques, such as zooming and distortions.\n",
    "\n",
    "It could also be that this accuracy is the limit of the SVM approach. [This paper](https://iopscience.iop.org/article/10.1088/1742-6596/1004/1/012001/meta) reported small (less than 0.5%) accuracy improvement when data augmentation is used with an SVM classifier. It may be time to switch to another classifier.\n",
    "\n",
    "This approach is also not scalable. It creates the entire augmented dataset in memory. Production code should store the augmented dataset on disk and use a [generator function](https://wiki.python.org/moin/Generators) to return them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sf9JB_ntOKTg"
   },
   "source": [
    "## Bonus opportunity 3 (50 points)\n",
    "\n",
    "Write code to incorporate face detection capabilities (see \"Face Detection Pipeline\" in the textbook), improve it to include non-maximal suppression (to produce 'clean' detection results) and demonstrate how it can be used to:\n",
    "- load an image that the model has never seen before (e.g. an image you downloaded from the Internet)\n",
    "- locate (i.e. detect) the face in the image\n",
    "- resize the face region to 62 $\\times$ 47 pixels\n",
    "- run the face recognition code you wrote above and produce a message showing the closest 'celebrity' from the dataset.\n",
    "\n",
    "Publish the code, the results, and document your steps and the rationale behind them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOZPjis7Phf"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution uses OpenCV face detection with Haar Cascades.\n",
    "\n",
    "This approach was chosen because OpenCV is a well-known image-processing framework. I wanted to be more familiar with it.\n",
    "\n",
    "Sources of information used to build this solution:\n",
    "\n",
    "\n",
    "* [How sliding windows for object detection works](https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/), the general concept of sliding a window through an image to detect objects, with Python and OpenCV code.\n",
    "* [How Haar Cascade works](http://www.willberger.org/cascade-haar-explained/), including an an animation of the Haar cascades working through the image segments.\n",
    "* [OpenCV official face detection tutorial](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html).\n",
    "* [How the face detection bounding box is determined](https://answers.opencv.org/question/177106/face-detection-bounding-box/).\n",
    "\n",
    "\n",
    "Note: this solution does not use non-maximum supression for lack of time and because OpenCV's face detector works fairly well out of the box already. We could run the detector with different parameters and apply OpenCV's own non-maximum suppression calculater, [`NMSBoxes()`](https://docs.opencv.org/master/d6/d0f/group__dnn.html#ga9d118d70a1659af729d01b10233213ee), afterwards. [This article](https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/) also describes an NMS solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 Load and preprocess the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this step is to load the image and convert it to grayscale (the format used by OpenCV face detection).\n",
    "\n",
    "[Attribution for the image used in these tests](https://archive.defense.gov/news/newsarticle.aspx?id=49508)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"./data/George-W-Bush3.jpeg\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a peek at the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, cmap=None):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(gray, \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Detect face(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: although the code detects multiple faces, it will work on the first face it finds. In production code we should process all the (possible) faces the code finds. To help with the assumption that we have only one face, `minSize` is set to a large value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we don't have to download the XML file (commonly done in tutorials out there)\n",
    "# opencv-python ships with the files; we just need to load them\n",
    "faceCascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "# See https://stackoverflow.com/questions/20801015/recommended-values-for-opencv-detectmultiscale-parameters/20805153#20805153\n",
    "# for an explanation of the parameters\n",
    "# And https://stackoverflow.com/questions/22249579/opencv-detectmultiscale-minneighbors-parameter/22250382#22250382\n",
    "# specifically for `minNeighbors`\n",
    "detected_faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.05,\n",
    "    minNeighbors=5,\n",
    "    # Use a large minimum size to avoid false positives\n",
    "    # Helps with our assumption that the first face is the main one\n",
    "    minSize=(100, 100),\n",
    "    flags=cv2.CASCADE_SCALE_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found a face at {}\".format(detected_faces[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 Visualize the face(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is not strictly necessary for detection, but it is a good way  to visualize what the face detection algorithm has found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert from OpenCV BGR to Matplotlib RGB\n",
    "# https://stackoverflow.com/a/54959575\n",
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "for x, y, w, h in detected_faces:\n",
    "    cv2.rectangle(rgb_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "show_image(rgb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 Extract the face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the first face we found and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = detected_faces[0]\n",
    "face = gray[y : y + h, x : x + w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(face, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6 Resize the extracted face to match classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize the image to match what the classifier was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "\n",
    "# What the classifier was trained on\n",
    "HEIGHT = 62\n",
    "WIDTH = 47\n",
    "\n",
    "test_image = transform.resize(face, (HEIGHT, WIDTH), anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(test_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7 Classify the face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the classifier we built above to predict the person's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from OpenCV 0.0-1.0 grayscale to the 0.0-255.0 used in the classifier\n",
    "f = test_image.ravel() * 255\n",
    "\n",
    "pred = svcclf.predict([f])\n",
    "print(\"Predicted: {}\".format(faces.target_names[pred][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLi0m9uuKkpD"
   },
   "source": [
    "## Conclusions (20 points)\n",
    "\n",
    "Write your conclusions and make sure to address the issues below:\n",
    "- What have you learned from this assignment?\n",
    "- Which parts were the most fun, time-consuming, enlightening, tedious?\n",
    "- What would you do if you had an additional week to work on this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmRWLHo5K99F"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ug_U9rd3K99G"
   },
   "source": [
    "### What have you learned from this assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use DataFrame `describe()` to view summary statistics at a glance. All the important values are available with one function call.\n",
    "* How much we get out of the box from the `ydata-profiling` package. It is like a \"mini EDA\" with one line of code.\n",
    "* Use the `verbose` parameter to follow the progress of long-running scikit-learn tasks.\n",
    "* Pay attention to `random_state` in the scikit-learn APIs to get consistent results.\n",
    "* Use `GridSearchCV()` to find parameters for a classifier.\n",
    "* The power and simplicity of Naive Bayes classifiers, even for seemly complex tasks such as digit classification. It can be used as a baseline before attempting more complex solutions.\n",
    "* How to use seaborn's heatmap for confusion matrix visualization. More specifically, the trick to zero out the diagonal with NumPy `fill_diagonal()` to make the mistakes stand out in the graph.\n",
    "* How surprisingly good random forest classifiers perform, achieving 97% in the digit classification without much work. Another case of \"try this before pulling your neural network card\" case. Especially with the emphasis on _explainable AI_, random forests may have an edge because even laymen can understand them.\n",
    "* The small number of components we need to explain variability (the PCA section).\n",
    "* Finally getting a chance to play with OpenCV and see first-hand how easy and feature-rich it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which parts were the most fun, time-consuming, enlightening, tedious?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time-consuming**\n",
    "\n",
    "* Understand the shape of the input data scikit-learn needs in the APIs. Thankfully the _Python Data Science Handbook_ did a good job with the code samples.\n",
    "* Image augmentation by hand. I'm sure there is a library out there for this...\n",
    "\n",
    "**Fun**\n",
    "\n",
    "* All of it. I'm amazed people get paid to do this stuff. Very few other legal things are this fun.\n",
    "\n",
    "**Enlightening**\n",
    "\n",
    "* Several items listed in the \"what have you learned\" section. If I had to pick the top three, they would be:\n",
    "    * The high accuracy of random forests.\n",
    "    * PCA: the small number of components we need to explain much of the variability in a dataset.\n",
    "    * How much can be achieved with a few lines of OpenCV code.\n",
    "* The SVM classifier not improving with data augmentation.\n",
    "\n",
    "**Tedious**\n",
    "\n",
    "* None. I would skip sleep, eating and, reluctantly, interacting with my loved ones, to do more of this. The whole course was a blast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What would you do if you had an additional week to work on this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get better at data augmentation, to deal with imbalanced datasets.\n",
    "* Work on more complex face detection tasks, e.g. multiple faces in the same picture."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Sf9JB_ntOKTg"
   ],
   "name": "CAP5768_Assignment1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
