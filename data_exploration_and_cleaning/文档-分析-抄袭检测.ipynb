{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plagiarism Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本查重\n",
    "\n",
    "文本查重，也称为文本去重（Plagiarism Detection），是一项旨在识别文本文档之间的相似性或重复性的技术或任务。\n",
    "\n",
    "它的主要目标是确定一个文本文档是否包含与其他文档相似或重复的内容，通常是为了检测抄袭、重复、剽窃等不当行为。\n",
    "\n",
    "文本查重的重要性和应用领域\n",
    "\n",
    "文本查重在今天的信息时代具有重要性，并在多个应用领域中发挥关键作用。以下是文本查重的重要性以及一些主要应用领域：\n",
    "\n",
    "1. 学术研究和教育领域\n",
    "\n",
    "   - 抄袭检测：在学术研究中，文本查重用于检测学生论文、学术论文和研究报告中的抄袭行为，以确保学术诚实性。\n",
    "   - 学术评估：学术评估机构和期刊使用文本查重来验证论文的原创性，以确保高质量的学术出版物。\n",
    "\n",
    "2. 新闻和媒体领域\n",
    "\n",
    "   - 新闻稿件验证：新闻编辑和出版商使用文本查重来验证新闻稿件的原创性，以避免不实报道和抄袭。\n",
    "   - 内容质量控制：维护在线新闻和媒体平台上的高质量内容，以提供可信赖的信息。\n",
    "\n",
    "3. 内容管理和版权保护\n",
    "\n",
    "   - 网站内容管理：网站管理员使用文本查重来管理网站上的重复内容，提供更好的用户体验。\n",
    "   - 版权保护：内容创作者和版权持有者使用文本查重来监测和保护其知识产权。\n",
    "\n",
    "4. 搜索引擎和信息检索\n",
    "\n",
    "   - 搜索结果提升：搜索引擎公司使用文本查重来消除重复内容，从而提高搜索结果的质量。\n",
    "   - 搜索引擎优化：网站管理员使用文本查重来改进其内容，以提高在搜索引擎中的排名。\n",
    "\n",
    "5. 法律和知识产权领域\n",
    "\n",
    "   - 知识产权保护：律师和知识产权专业人员使用文本查重来监测和保护专利、商标和版权等知识产权。\n",
    "   - 法庭证据：文本查重用于法庭案件中，以确定证据是否存在抄袭或知识产权侵权。\n",
    "\n",
    "6. 广告和市场营销\n",
    "\n",
    "   - 广告监管：广告行业使用文本查重来验证广告内容的原创性，以确保合规性和消费者保护。\n",
    "   - 品牌声誉：企业使用文本查重来监测和保护其品牌声誉，以避免负面广告。\n",
    "\n",
    "总的来说，文本查重在多个领域中都具有广泛的应用，以确保内容的原创性、知识产权的保护、信息质量的提高和法律合规性的维护。它有助于维护信任、保护知识产权和提供更高质量的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 余弦相似度\n",
    "\n",
    "余弦相似度 (Cosine Similarity)是一种常用的文本相似性度量方法，用于比较两个文本向量之间的夹角。\n",
    "\n",
    "具体来说，余弦相似度度量了两个文本向量之间的夹角余弦值，值越接近1表示文本越相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.79056942 0.54772256]\n",
      " [0.79056942 1.         0.4330127 ]\n",
      " [0.54772256 0.4330127  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "documents = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(documents)\n",
    "cosine_sim = cosine_similarity(vectors)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "data = [\"This is some sample data\",\n",
    "        \"This is some example data\",\n",
    "        \"This is some example data\",\n",
    "        \"More sample data\",\n",
    "        \"Another example\"]\n",
    "# 数据矢量化\n",
    "vectorizer = TfidfVectorizer ()\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "# 计算余弦相似度\n",
    "similarities = cosine_similarity(vectors)\n",
    "# 查找重复\n",
    "duplicates = set ()\n",
    "for i in range (len(similarities)):\n",
    "    for j in range(i+1, len(similarities)):\n",
    "        if similarities[i][j] > 0.95:\n",
    "            duplicates.add (j)\n",
    "# 删除重复内容\n",
    "for i in sorted (duplicates, reverse=True):\n",
    "    del data[i]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过滤相似的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from itertools import combinations \n",
    " \n",
    " \n",
    "# Set globals \n",
    "nlp = spacy.load(\"en_core_web_md\") \n",
    " \n",
    "def pre_process(titles): \n",
    "    \"\"\" \n",
    "    Pre-processes titles by removing stopwords and lemmatizing text. \n",
    "    :param titles: list of strings, contains target titles,. \n",
    "    :return: preprocessed_title_docs, list containing pre-processed titles. \n",
    "    \"\"\" \n",
    " \n",
    "    # Preprocess all the titles \n",
    "    title_docs = [nlp(x) for x in titles] \n",
    "    preprocessed_title_docs = [] \n",
    "    lemmatized_tokens = [] \n",
    "    for title_doc in title_docs: \n",
    "        for token in title_doc: \n",
    "            if not token.is_stop: \n",
    "                lemmatized_tokens.append(token.lemma_) \n",
    "        preprocessed_title_docs.append(\" \".join(lemmatized_tokens)) \n",
    "        del lemmatized_tokens[ \n",
    "            : \n",
    "            ]  # empty the lemmatized tokens list as the code moves onto a new title \n",
    " \n",
    "    return preprocessed_title_docs \n",
    " \n",
    "def similarity_filter(titles): \n",
    "    \"\"\" \n",
    "    Recursively check if titles pass a similarity filter. \n",
    "    :param titles: list of strings, contains titles. \n",
    "    If the function finds titles that fail the similarity test, the above param will be the function output. \n",
    "    :return: this method upon itself unless there are no similar titles; in that case the feed that was passed \n",
    "    in is returned. \n",
    "    \"\"\" \n",
    " \n",
    "    # Preprocess titles \n",
    "    preprocessed_title_docs = pre_process(titles) \n",
    " \n",
    "    # Remove similar titles \n",
    "    all_summary_pairs = list(combinations(preprocessed_title_docs, 2)) \n",
    "    similar_titles = [] \n",
    "    for pair in all_summary_pairs: \n",
    "        title1 = nlp(pair[0]) \n",
    "        title2 = nlp(pair[1]) \n",
    "        similarity = title1.similarity(title2) \n",
    "        if similarity > 0.8: \n",
    "            similar_titles.append(pair) \n",
    " \n",
    "    titles_to_remove = [] \n",
    "    for a_title in similar_titles: \n",
    "        # Get the index of the first title in the pair \n",
    "        index_for_removal = preprocessed_title_docs.index(a_title[0]) \n",
    "        titles_to_remove.append(index_for_removal) \n",
    " \n",
    "    # Get indices of similar titles and remove them \n",
    "    similar_title_counts = set(titles_to_remove) \n",
    "    similar_titles = [ \n",
    "        x[1] for x in enumerate(titles) if x[0] in similar_title_counts \n",
    "    ] \n",
    " \n",
    "    # Exit the recursion if there are no longer any similar titles \n",
    "    if len(similar_title_counts) == 0: \n",
    "        return titles \n",
    " \n",
    "    # Continue the recursion if there are still titles to remove \n",
    "    else: \n",
    "        # Remove similar titles from the next input \n",
    "        for title in similar_titles: \n",
    "            idx = titles.index(title) \n",
    "            titles.pop(idx) \n",
    " \n",
    "        return similarity_filter(titles) \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    your_title_list = ['title1', 'title2'] \n",
    "    similarty_filter(your_title_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编辑距离\n",
    "\n",
    "编辑距离（Levenshtein Distance）是衡量两个字符串之间差异的一种方法，即将一个字符串转换为另一个字符串所需的最小单字符编辑操作（插入、删除或替换）次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_levenshtein_distance(text1, text2):\n",
    "    m, n = len(text1), len(text2)\n",
    "    dp = np.zeros((m + 1, n + 1))\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if text1[i - 1] == text2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "text1 = \"I love Python programming\"\n",
    "text2 = \"Python programming is great\"\n",
    "\n",
    "levenshtein_distance = calculate_levenshtein_distance(text1, text2)\n",
    "print(levenshtein_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FuzzyWuzzy \n",
    "\n",
    "https://github.com/seatgeek/thefuzz\n",
    "\n",
    "是一个简单易用的模糊字符串匹配工具包。它依据 Levenshtein Distance 算法，计算两个序列之间的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从一个example.csv的CSV文件中读取数据，使用FuzzyWuzzy库中的process.extractOne方法提取与输入值最相似的字符串。如果相似度超过80%，就认为找到了正确的清洗结果，并将这些结果保存到新的cleaned_data.csv文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fuzzywuzzy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 从CSV文件中读取数据\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "# 从CSV文件中读取数据\n",
    "data = pd.read_csv(\"example.csv\")\n",
    "# 创建一个空的DataFrame来存储结果\n",
    "results = pd.DataFrame(columns=[\"Original\", \"Cleaned\"])\n",
    "# 使用FuzzyWuzzy对数据进行清洗\n",
    "for index, row in data.iterrows():\n",
    "    cleaned_value = process.extractOne(row['value'], ['a', 'b', 'c', 'd'], score_cutoff=80)\n",
    "    results = results.append({\"Original\": row['value'], \"Cleaned\": cleaned_value[0]}, ignore_index=True)\n",
    "# 将结果保存到新的CSV文件\n",
    "results.to_csv(\"cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义了一个计算字符串相似度的函数calculate_similarity，然后我们使用多进程来并行计算多个字符串对的相似度，这能够显著提升大规模数据处理的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from time import time\n",
    "def calculate_similarity(str1, str2):\n",
    "    \"\"\"计算两个字符串的相似度\"\"\"\n",
    "    start_time = time()\n",
    "    result = fuzz.ratio(str1, str2)\n",
    "    end_time = time()\n",
    "    return result, end_time - start_time\n",
    "str1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "str2 = \"A quick, brown dog outpaces a lazy fox.\"\n",
    "# 计算相似度\n",
    "similarity, time_taken = calculate_similarity(str1, str2)\n",
    "print(f\"Similarity: {similarity}\")\n",
    "print(f\"Time taken: {time_taken} seconds\")\n",
    "# 性能优化：使用并行处理\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "def parallel_calculate_similarity(input_pair):\n",
    "    str1, str2 = input_pair\n",
    "    return calculate_similarity(str1, str2)\n",
    "str_list1 = [str1] * multiprocessing.cpu_count()\n",
    "str_list2 = [str2] * multiprocessing.cpu_count()\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(parallel_calculate_similarity, zip(str_list1, str_list2)))\n",
    "# 输出并行处理结果\n",
    "for result in results:\n",
    "    print(f\"Similarity: {result[0]}, Time taken: {result[1]:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
