{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评估\n",
    "\n",
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as skmet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pprint\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估措施分类\n",
    "\n",
    "evaluation measures classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option('display.width', 75)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n",
    "\n",
    "feature_cols = ['satverbal','satmath','gpaoverall',\n",
    "  'parentincome','gender']\n",
    "\n",
    "# separate NLS data into train and test datasets\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(nls97compba[feature_cols],\\\n",
    "  nls97compba[['completedba']], test_size=0.3, random_state=0)\n",
    "      \n",
    "# do one hot encoding and scaling\n",
    "ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n",
    "ohe.fit(X_train)\n",
    "X_train_enc, X_test_enc = \\\n",
    "  ohe.transform(X_train), ohe.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standcols = X_train_enc.iloc[:,:-1].columns\n",
    "scaler.fit(X_train_enc[standcols])\n",
    "X_train_enc = \\\n",
    "  pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n",
    "  columns=standcols, index=X_train_enc.index).\\\n",
    "  join(X_train_enc[['gender_Female']], how=\"left\", on=None, validate=\"many_to_many\")\n",
    "X_test_enc = \\\n",
    "  pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n",
    "  columns=standcols, index=X_test_enc.index).\\\n",
    "  join(X_test_enc[['gender_Female']], how=\"left\", on=None, validate=\"many_to_many\")\n",
    "\n",
    "\n",
    "# k nearest neighbor classification\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_enc, y_train.values.ravel())\n",
    "pred = knn.predict(X_test_enc)\n",
    "\n",
    "# show a confusion matrix\n",
    "cm = skmet.confusion_matrix(y_test, pred, labels=knn.classes_)\n",
    "cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm, \\\n",
    " display_labels=['Negative--No BA', 'Positive--Completed BA'])\n",
    "cmplot.plot()\n",
    "cmplot.ax_.set(title='Confusion Matrix', \n",
    "  xlabel='Predicted Value', ylabel='Actual Value')\n",
    "\n",
    "# calculate accuracy, sensitivity, specificity, and precision\n",
    "tn, fp, fn, tp = skmet.confusion_matrix(y_test.values.ravel(), pred).ravel()\n",
    "tn, fp, fn, tp\n",
    "accuracy = (tp + tn) / pred.shape[0]\n",
    "accuracy\n",
    "sensitivity = tp / (tp + fn)\n",
    "sensitivity\n",
    "specificity = tn / (tn+fp)\n",
    "specificity\n",
    "precision = tp / (tp + fp)\n",
    "precision\n",
    "\n",
    "print(\"accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f\"  %\n",
    "  (skmet.accuracy_score(y_test.values.ravel(), pred),\n",
    "  skmet.recall_score(y_test.values.ravel(), pred),\n",
    "  skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),\n",
    "  skmet.precision_score(y_test.values.ravel(), pred)))\n",
    "\n",
    "\n",
    "# try this with a random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=2, \n",
    "  n_jobs=-1, random_state=0)\n",
    "rfc.fit(X_train_enc, y_train.values.ravel())\n",
    "pred = rfc.predict(X_test_enc)\n",
    "tn, fp, fn, tp = skmet.confusion_matrix(y_test.values.ravel(), pred).ravel()\n",
    "tn, fp, fn, tp\n",
    "\n",
    "print(\"accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f\"  %\n",
    "  (skmet.accuracy_score(y_test.values.ravel(), pred),\n",
    "  skmet.recall_score(y_test.values.ravel(), pred),\n",
    "  skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),\n",
    "  skmet.precision_score(y_test.values.ravel(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAP/ROC曲线\n",
    "\n",
    "CAP ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option('display.width', 75)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "# a playing cards example\n",
    "numobs = 6\n",
    "inclasscnt = 3\n",
    "plt.yticks([1,2,3])\n",
    "plt.plot([0, numobs], [0, inclasscnt], c = 'b', label = 'Random Model')\n",
    "plt.plot([0, inclasscnt, numobs], [0, inclasscnt, inclasscnt], c = 'grey', linewidth = 2, label = 'Perfect Model')\n",
    "plt.title(\"Cumulative Accuracy Profile\")\n",
    "plt.xlabel(\"Total Cards\")\n",
    "plt.ylabel(\"In-class (Red) Cards\")\n",
    "plt.legend()\n",
    "\n",
    "nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n",
    "\n",
    "feature_cols = ['satverbal','satmath','gpaoverall',\n",
    "  'parentincome','gender']\n",
    "\n",
    "# separate NLS data into train and test datasets\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(nls97compba[feature_cols],\\\n",
    "  nls97compba[['completedba']], test_size=0.3, random_state=0)\n",
    "      \n",
    "# do one hot encoding and scaling\n",
    "ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n",
    "ohe.fit(X_train)\n",
    "X_train_enc, X_test_enc = \\\n",
    "  ohe.transform(X_train), ohe.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standcols = X_train_enc.iloc[:,:-1].columns\n",
    "scaler.fit(X_train_enc[standcols])\n",
    "X_train_enc = \\\n",
    "  pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n",
    "  columns=standcols, index=X_train_enc.index).\\\n",
    "  join(X_train_enc[['gender_Female']], how=\"left\", on=None, validate=\"many_to_many\")\n",
    "X_test_enc = \\\n",
    "  pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n",
    "  columns=standcols, index=X_test_enc.index).\\\n",
    "  join(X_test_enc[['gender_Female']], how=\"left\", on=None, validate=\"many_to_many\")\n",
    "\n",
    "\n",
    "# random forest classification for feature importance\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=2, \n",
    "  n_jobs=-1, random_state=0)\n",
    "\n",
    "# plot random model and perfect model\n",
    "numobs = y_test.shape[0]\n",
    "inclasscnt = y_test.iloc[:,0].sum()\n",
    "\n",
    "plt.plot([0, numobs], [0, inclasscnt], c = 'b', label = 'Random Model')\n",
    "plt.plot([0, inclasscnt, numobs], [0, inclasscnt, inclasscnt], c = 'grey', linewidth = 2, label = 'Perfect Model')\n",
    "plt.axvline(numobs/2, color='black', linestyle='dashed', linewidth=1)\n",
    "plt.axhline(numobs/2, color='black', linestyle='dashed', linewidth=1)\n",
    "plt.title(\"Cumulative Accuracy Profile\")\n",
    "plt.xlabel(\"Total Observations\")\n",
    "plt.ylabel(\"In-class Observations\")\n",
    "plt.legend()\n",
    "\n",
    "# plot k nearest neighbor and random forest models\n",
    "def addplot(model, X, Xtest, y, modelname, linecolor):\n",
    "  model.fit(X, y.values.ravel())\n",
    "  probs = model.predict_proba(Xtest)[:, 1]\n",
    "  \n",
    "  probdf = pd.DataFrame(zip(probs, y_test.values.ravel()),\n",
    "    columns=(['prob','inclass']))\n",
    "  probdf.loc[-1] = [0,0]\n",
    "  probdf = probdf.sort_values(['prob','inclass'],\n",
    "    ascending=False).\\\n",
    "    assign(inclasscum = lambda x: x.inclass.cumsum())\n",
    "  inclassmidpoint = probdf.iloc[int(probdf.shape[0]/2)].\\\n",
    "    inclasscum\n",
    "  plt.axhline(inclassmidpoint, color=linecolor,\n",
    "    linestyle='dashed', linewidth=1)\n",
    "  plt.plot(np.arange(0, probdf.shape[0]),\n",
    "    probdf.inclasscum, c = linecolor,\n",
    "    label = modelname, linewidth = 4)\n",
    "\n",
    "addplot(knn, X_train_enc, X_test_enc, y_train,\n",
    "  'KNN', 'red')\n",
    "addplot(rfc, X_train_enc, X_test_enc, y_train,\n",
    "  'Random Forest', 'green')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# plot probability distribution\n",
    "rfc.fit(X_train_enc, y_train.values.ravel())\n",
    "pred = rfc.predict(X_test_enc)\n",
    "pred_probs = rfc.predict_proba(X_test_enc)[:, 1]\n",
    "\n",
    "probdf = pd.DataFrame(zip(pred_probs, pred, y_test.values.ravel()),\n",
    "  columns=(['prob','pred','actual']))\n",
    "\n",
    "probdf.groupby(['pred'])['prob'].agg(['min','max'])\n",
    "\n",
    "sb.kdeplot(probdf.loc[probdf.actual==1].prob, shade=True, color='red',\n",
    "  label=\"Completed BA\")\n",
    "sb.kdeplot(probdf.loc[probdf.actual==0].prob, shade=True, color='green',\n",
    "  label=\"Did Not Complete\")\n",
    "plt.axvline(0.5, color='black', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(0.65, color='black', linestyle='dashed', linewidth=1)\n",
    "plt.title(\"Predicted Probability Distribution\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, ths = skmet.roc_curve(y_test, pred_probs)\n",
    "ths = ths[1:]\n",
    "fpr = fpr[1:]\n",
    "tpr = tpr[1:]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ths, fpr, label=\"False Positive Rate\")\n",
    "ax.plot(ths, tpr, label=\"Sensitivity\")\n",
    "ax.set_title('False Positive Rate and Sensitivity by Threshold')\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('False Positive Rate and Sensitivity')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, linewidth=4, color=\"black\")\n",
    "ax.set_title('ROC curve')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "\n",
    "tholdind = np.where((ths>0.499) & (ths<0.501))[0][0]\n",
    "tholdindlow = np.where((ths>0.397) & (ths<0.404))[0][0]\n",
    "tholdindhigh = np.where((ths>0.599) & (ths<0.601))[0][0]\n",
    "plt.vlines((fpr[tholdindlow],fpr[tholdind],fpr[tholdindhigh]),\n",
    "  0, 1, linestyles =\"dashed\", colors =[\"green\",\"blue\",\"purple\"])\n",
    "plt.hlines((tpr[tholdindlow],tpr[tholdind],tpr[tholdindhigh]),\n",
    "  0, 1, linestyles =\"dashed\", colors =[\"green\",\"blue\",\"purple\"])\n",
    "\n",
    "\n",
    "# plot precision and sensitivity lines\n",
    "prec, sens, ths = skmet.precision_recall_curve(y_test, pred_probs)\n",
    "\n",
    "prec = prec[1:-10]\n",
    "sens = sens[1:-10]\n",
    "ths  = ths[:-10]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ths, prec, label='Precision')\n",
    "ax.plot(ths, sens, label='Sensitivity')\n",
    "ax.set_title('Precision and Sensitivity by Threshold')\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Precision and Sensitivity')\n",
    "ax.set_xlim(0.3,0.9)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# plot precision and sensitivity curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(sens, prec)\n",
    "ax.set_title('Precision-Sensitivity Curve')\n",
    "ax.set_xlabel('Sensitivity')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_ylim(0)\n",
    "plt.yticks(np.arange(0.2, 0.9, 0.2))\n",
    "\n",
    "temp = pd.DataFrame(zip(sens, prec, ths), columns=['sensitivity','precision','threshold'])\n",
    "temp.describe()\n",
    "temp.head(70)\n",
    "temp.tail(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多级评价\n",
    "\n",
    "evaluation multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 75)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "nls97degreelevel = pd.read_csv(\"data/nls97degreelevel.csv\")\n",
    "\n",
    "feature_cols = ['satverbal','satmath','gpaoverall',\n",
    "  'parentincome','gender']\n",
    "\n",
    "# separate NLS data into train and test datasets\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(nls97degreelevel[feature_cols],\\\n",
    "  nls97degreelevel[['degreelevel']], test_size=0.3, random_state=0)\n",
    "      \n",
    "# do one hot encoding and scaling\n",
    "ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n",
    "ohe.fit(X_train)\n",
    "X_train_enc, X_test_enc = \\\n",
    "  ohe.transform(X_train), ohe.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standcols = X_train_enc.iloc[:,:-1].columns\n",
    "scaler.fit(X_train_enc[standcols])\n",
    "X_train_enc = \\\n",
    "  pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n",
    "  columns=standcols, index=X_train_enc.index).\\\n",
    "  join(X_train_enc[['gender_Female']], how=\"left\", on=None, validate=\"many_to_many\")\n",
    "X_test_enc = \\\n",
    "  pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n",
    "  columns=standcols, index=X_test_enc.index).\\\n",
    "  join(X_test_enc[['gender_Female']], how=\"left\", on=None, validate=\"many_to_many\")\n",
    "\n",
    "\n",
    "# k nearest neighbor classification model\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_enc, y_train.values.ravel())\n",
    "pred = knn.predict(X_test_enc)\n",
    "pred_probs = knn.predict_proba(X_test_enc)[:, 1]\n",
    "\n",
    "# create a confusion matrix\n",
    "cm = skmet.confusion_matrix(y_test, pred)\n",
    "cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['High School', 'Bachelor','Post-Graduate'])\n",
    "cmplot.plot()\n",
    "cmplot.ax_.set(title='Confusion Matrix', \n",
    "  xlabel='Predicted Value', ylabel='Actual Value')\n",
    "\n",
    "print(skmet.classification_report(y_test, pred,\n",
    "  target_names=['High School', 'Bachelor','Post-Graduate']))\n",
    "\n",
    "fpr, tpr, ths = skmet.roc_curve(y_test, pred_probs)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, linewidth=4, color=\"black\")\n",
    "ax.set_title('ROC curve')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "\n",
    "nls97compba.dtypes\n",
    "nls97 = pd.read_csv(\"data/nls97f.csv\")\n",
    "nls97.dtypes\n",
    "\n",
    "nls97.highestdegree.value_counts().sort_index()\n",
    "\n",
    "nls97['degreelevel'] = \\\n",
    "  np.where(nls97.highestdegree.isnull(),np.nan,\n",
    "  np.where(nls97.highestdegree.str[0:1].isin(['2','3']),2,\n",
    "  np.where(nls97.highestdegree.str[0:1].isin(['4']),3,\n",
    "  np.where(nls97.highestdegree.str[0:1].isin(['5','6','7']),4,1))))\n",
    "\n",
    "nls97.groupby(['degreelevel','highestdegree']).size().reset_index()\n",
    "\n",
    "nls97.degreelevel.value_counts()\n",
    "\n",
    "nls97degreelevel = pd.merge(nls97compba, nls97[['personid','degreelevel']], left_on=['personid'], right_on=['personid'], how=\"left\")\n",
    "nls97degreelevel = nls97degreelevel.loc[nls97degreelevel.degreelevel>1]\n",
    "nls97degreelevel['degreelevel'] = nls97degreelevel.degreelevel-1\n",
    "nls97degreelevel.groupby(['degreelevel','completedba']).size().reset_index()\n",
    "\n",
    "nls97degreelevel.to_csv(\"data/nls97degreelevel.csv\")\n",
    "\n",
    "# plot random model and perfect model\n",
    "numobs = y_test.shape[0]\n",
    "inclasscnt = y_test.iloc[:,0].sum()\n",
    "\n",
    "plt.plot([0, numobs], [0, inclasscnt], c = 'b', label = 'Random Model')\n",
    "plt.plot([0, inclasscnt, numobs], [0, inclasscnt, inclasscnt], c = 'grey', linewidth = 2, label = 'Perfect Model')\n",
    "plt.axvline(numobs/2, color='black', linestyle='dashed', linewidth=1)\n",
    "plt.axhline(numobs/2, color='black', linestyle='dashed', linewidth=1)\n",
    "plt.title(\"Cumulative Accuracy Profile\")\n",
    "plt.xlabel(\"Total Observations\")\n",
    "plt.ylabel(\"In-class Observations\")\n",
    "plt.legend()\n",
    "\n",
    "# plot k nearest neighbor and random forest models\n",
    "def addplot(model, X, Xtest, y, modelname, linecolor):\n",
    "  model.fit(X, y.values.ravel())\n",
    "  probs = model.predict_proba(Xtest)[:, 1]\n",
    "  \n",
    "  probdf = pd.DataFrame(zip(probs, y_test.values.ravel()),\n",
    "    columns=(['prob','inclass']))\n",
    "  probdf.loc[-1] = [0,0]\n",
    "  probdf = probdf.sort_values(['prob','inclass'],\n",
    "    ascending=False).\\\n",
    "    assign(inclasscum = lambda x: x.inclass.cumsum())\n",
    "  inclassmidpoint = probdf.iloc[int(probdf.shape[0]/2)].\\\n",
    "    inclasscum\n",
    "  plt.axhline(inclassmidpoint, color=linecolor,\n",
    "    linestyle='dashed', linewidth=1)\n",
    "  plt.plot(np.arange(0, probdf.shape[0]),\n",
    "    probdf.inclasscum, c = linecolor,\n",
    "    label = modelname, linewidth = 4)\n",
    "\n",
    "addplot(knn, X_train_enc, X_test_enc, y_train,\n",
    "  'KNN', 'red')\n",
    "addplot(rfc, X_train_enc, X_test_enc, y_train,\n",
    "  'Random Forest', 'green')\n",
    "plt.legend()\n",
    "\n",
    "# plot probability distribution\n",
    "knn.fit(X_train_enc, y_train.values.ravel())\n",
    "pred = knn.predict(X_test_enc)\n",
    "cm = skmet.confusion_matrix(y_test, pred)\n",
    "cm\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, ths = skmet.roc_curve(y_test, pred_probs)\n",
    "ths = ths[1:]\n",
    "fpr = fpr[1:]\n",
    "tpr = tpr[1:]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ths, fpr, label=\"False Positive Rate\")\n",
    "ax.plot(ths, tpr, label=\"Sensitivity\")\n",
    "ax.set_title('False Positive Rate and Sensitivity by Threshold')\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('False Positive Rate and Sensitivity')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, linewidth=4, color=\"black\")\n",
    "ax.set_title('ROC curve')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "\n",
    "tholdind = np.where((ths>0.499) & (ths<0.501))[0][0]\n",
    "tholdindlow = np.where((ths>0.397) & (ths<0.404))[0][0]\n",
    "tholdindhigh = np.where((ths>0.599) & (ths<0.601))[0][0]\n",
    "plt.vlines((fpr[tholdindlow],fpr[tholdind],fpr[tholdindhigh]),\n",
    "  0, 1, linestyles =\"dashed\", colors =[\"green\",\"blue\",\"purple\"])\n",
    "plt.hlines((tpr[tholdindlow],tpr[tholdind],tpr[tholdindhigh]),\n",
    "  0, 1, linestyles =\"dashed\", colors =[\"green\",\"blue\",\"purple\"])\n",
    "\n",
    "\n",
    "# plot precision and sensitivity lines\n",
    "sens, prec, ths = skmet.precision_recall_curve(y_test, pred_probs)\n",
    "sens = sens[1:-10]\n",
    "prec = prec[1:-10]\n",
    "ths  = ths[:-10]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ths, prec, label='Precision')\n",
    "ax.plot(ths, sens, label='Sensitivity')\n",
    "ax.set_title('Precision and Sensitivity by Threshold')\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Precision and Sensitivity')\n",
    "ax.set_xlim(0.3,0.9)\n",
    "ax.legend()\n",
    "\n",
    "# plot precision and sensitivity curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(sens, prec)\n",
    "ax.set_title('Precision-Sensitivity Curve')\n",
    "ax.set_xlabel('Sensitivity')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlim(0.5,0.9)\n",
    "\n",
    "temp = pd.DataFrame(zip(sens, prec, ths), columns=['sensitivity','precision','threshold'])\n",
    "temp.describe()\n",
    "temp.head(70)\n",
    "temp.tail(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归度量\n",
    "\n",
    "regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "# load the NLS data\n",
    "landtemps = pd.read_csv(\"data/landtemps2019avgs.csv\")\n",
    "\n",
    "feature_cols = ['latabs','elevation']\n",
    "\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(landtemps[feature_cols],\\\n",
    "  landtemps[['avgtemp']], test_size=0.3, random_state=0)\n",
    "      \n",
    "# scale the data  \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = \\\n",
    "  pd.DataFrame(scaler.transform(X_train),\n",
    "  columns=feature_cols, index=X_train.index)\n",
    "X_test = \\\n",
    "  pd.DataFrame(scaler.transform(X_test),\n",
    "  columns=feature_cols, index=X_test.index)\n",
    "\n",
    "scaler.fit(y_train)\n",
    "y_train, y_test = \\\n",
    "  pd.DataFrame(scaler.transform(y_train),\n",
    "  columns=['avgtemp'], index=y_train.index),\\\n",
    "  pd.DataFrame(scaler.transform(y_test),\n",
    "  columns=['avgtemp'], index=y_test.index)\n",
    "\n",
    "\n",
    "# use linear regression for recursive feature elimination\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "np.column_stack((lr.coef_.ravel(),X_test.columns.values))\n",
    "\n",
    "# get predictions and residuals\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "preddf = pd.DataFrame(pred, columns=['prediction'],\n",
    "  index=X_test.index).join(X_test, how=\"left\", on=None, validate=\"many_to_many\").join(y_test)\n",
    "preddf['resid'] = preddf.avgtemp-preddf.prediction\n",
    "\n",
    "preddf.resid.agg(['mean','median','skew','kurtosis'])\n",
    "\n",
    "# plot the residuals\n",
    "plt.hist(preddf.resid, color=\"blue\")\n",
    "plt.axvline(preddf.resid.mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.title(\"Histogram of Residuals for Temperature Model\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# plot predictions against the residuals\n",
    "plt.scatter(preddf.prediction, preddf.resid, color=\"blue\")\n",
    "plt.axhline(0, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.title(\"Scatterplot of Predictions and Residuals\")\n",
    "plt.xlabel(\"Predicted Temperature\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n",
    "\n",
    "# generate summary model evaluation statistics\n",
    "mse = skmet.mean_squared_error(y_test, pred)\n",
    "mse\n",
    "rmse = skmet.mean_squared_error(y_test, pred, squared=False)\n",
    "rmse\n",
    "mae = skmet.mean_absolute_error(y_test, pred)\n",
    "mae\n",
    "r2 = skmet.r2_score(y_test, pred)\n",
    "r2\n",
    "   \n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "mae = skmet.mean_absolute_error(y_test, pred)\n",
    "mae\n",
    "r2 = skmet.r2_score(y_test, pred)\n",
    "r2\n",
    "\n",
    "preddf = pd.DataFrame(pred, columns=['prediction'],\n",
    "  index=X_test.index).join(X_test).join(y_test, how=\"left\", on=None, validate=\"many_to_many\")\n",
    "preddf['resid'] = preddf.avgtemp-preddf.prediction\n",
    "\n",
    "# plot predictions against the residuals\n",
    "plt.scatter(preddf.prediction, preddf.resid, color=\"blue\")\n",
    "plt.axhline(0, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.title(\"Scatterplot of Predictions and Residuals with KNN Model\")\n",
    "plt.xlabel(\"Predicted Temperature\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证\n",
    "\n",
    "cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "# load the NLS data\n",
    "landtemps = pd.read_csv(\"data/landtemps2019avgs.csv\")\n",
    "\n",
    "feature_cols = ['latabs','elevation']\n",
    "\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(landtemps[feature_cols],\\\n",
    "  landtemps[['avgtemp']], test_size=0.1, random_state=0)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "      \n",
    "# use linear regression for recursive feature elimination\n",
    "def getscores(model):\n",
    "  pipeline = make_pipeline(StandardScaler(), model)\n",
    "  scores = cross_validate(pipeline, X=X_train, y=y_train,\n",
    "    cv=kf, scoring=['r2'], n_jobs=1)\n",
    "  scorelist.append(dict(model=str(model),\n",
    "    fit_time=scores['fit_time'].mean(),\n",
    "    r2=scores['test_r2'].mean()))\n",
    "\n",
    "\n",
    "scorelist = []\n",
    "getscores(LinearRegression())\n",
    "getscores(RandomForestRegressor(max_depth=2))\n",
    "getscores(KNeighborsRegressor(n_neighbors=5))\n",
    "\n",
    "scorelist\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "scores = cross_validate(pipeline, X=X_train, y=y_train,\n",
    "  cv=kf, scoring=['r2'], n_jobs=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网格搜索\n",
    "\n",
    "grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "# load the NLS data\n",
    "landtemps = pd.read_csv(\"data/landtemps2019avgs.csv\")\n",
    "\n",
    "feature_cols = ['latabs','elevation']\n",
    "\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(landtemps[feature_cols],\\\n",
    "  landtemps[['avgtemp']], test_size=0.3, random_state=0)\n",
    "      \n",
    "# X = landtemps[feature_cols]\n",
    "# y = landtemps[['avgtemp']]\n",
    "\n",
    "# use linear regression for recursive feature elimination\n",
    "lr = LinearRegression()\n",
    "split = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "pipe = Pipeline([\n",
    " ('scaler', StandardScaler()),\n",
    " ('lr', LinearRegression())\n",
    "        ])\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "scores = cross_validate(pipeline, X=X_train, y=y_train, cv=split, n_jobs=1, return_estimator = True)\n",
    "scores\n",
    "type(scores)\n",
    "temp = pipeline.fit(X_train, y_train)\n",
    "\n",
    "# get predictions and residuals\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "preddf = pd.DataFrame(pred, columns=['prediction'],\n",
    "  index=X_test.index).join(X_test).join(y_test)\n",
    "preddf['resid'] = preddf.avgtemp-preddf.prediction\n",
    "\n",
    "preddf.resid.agg(['mean','median','skew','kurtosis'])\n",
    "\n",
    "# generate summary model evaluation statistics\n",
    "mse = skmet.mean_squared_error(y_test, pred)\n",
    "mse\n",
    "rmse = skmet.mean_squared_error(y_test, pred, squared=False)\n",
    "rmse\n",
    "mae = skmet.mean_absolute_error(y_test, pred)\n",
    "mae\n",
    "r2 = skmet.r2_score(y_test, pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 管道预处理\n",
    "\n",
    "pipeline preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: -23755.52, R-squared: 0.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + \"/helperfunctions\")\n",
    "from preprocfunc import OutlierTrans\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "# load the NLS data and create training and testing DataFrames\n",
    "nls97wages = pd.read_csv(\"data/nls97wagesb.csv\")\n",
    "nls97wages.set_index(\"personid\", inplace=True)\n",
    "\n",
    "nls97wages.dropna(subset=['wageincome'], inplace=True)\n",
    "nls97wages.loc[nls97wages.motherhighgrade==95,\n",
    "  'motherhighgrade'] = np.nan\n",
    "nls97wages.loc[nls97wages.fatherhighgrade==95,\n",
    "  'fatherhighgrade'] = np.nan\n",
    "\n",
    "# setup the features and target\n",
    "num_cols = ['gpascience','gpaenglish','gpamath','gpaoverall',\n",
    "  'motherhighgrade','fatherhighgrade','parentincome']\n",
    "cat_cols = ['gender']\n",
    "bin_cols = ['completedba']\n",
    "\n",
    "nls97wages[['wageincome'] + num_cols].agg(['count','min','median','max']).T\n",
    "\n",
    "target = nls97wages[['wageincome']]\n",
    "features = nls97wages[num_cols + cat_cols + bin_cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(features,\\\n",
    "  target, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# setup pipelines for column transformation\n",
    "standtrans = make_pipeline(OutlierTrans(2),\n",
    "  StandardScaler())\n",
    "cattrans = make_pipeline(SimpleImputer(strategy=\"most_frequent\"),\n",
    "  OneHotEncoder(drop_last=True))\n",
    "bintrans = make_pipeline(SimpleImputer(strategy=\"most_frequent\"))\n",
    "\n",
    "coltrans = ColumnTransformer(\n",
    "  transformers=[\n",
    "    (\"stand\", standtrans, num_cols),\n",
    "    (\"cat\", cattrans, ['gender']),\n",
    "    (\"bin\", bintrans, ['completedba'])\n",
    "  ]\n",
    ")\n",
    "\n",
    "# add feature selection and a linear model to the pipeline and look at the parameter estimates\n",
    "lr = LinearRegression()\n",
    "\n",
    "pipe1 = make_pipeline(coltrans,\n",
    "  KNNImputer(n_neighbors=5), lr)\n",
    "\n",
    "ttr=TransformedTargetRegressor(regressor=pipe1,\n",
    "  transformer=StandardScaler())\n",
    "\n",
    "\n",
    "# run kfold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "scores = cross_validate(ttr, X=X_train, y=y_train,\n",
    "  cv=kf, scoring=('r2', 'neg_mean_absolute_error'),\n",
    "  n_jobs=1)\n",
    "\n",
    "print(\"Mean Absolute Error: %.2f, R-squared: %.2f\" % \n",
    "  (scores['test_neg_mean_absolute_error'].mean(),\n",
    "  scores['test_r2'].mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
