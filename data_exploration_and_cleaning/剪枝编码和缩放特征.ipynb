{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 剪枝编码和缩放特征\n",
    "\n",
    "Pruning Encoding and Rescaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 测试_培训_拆分\n",
    "\n",
    "testing training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 75)\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "nls97 = pd.read_csv(\"data/nls97b.csv\")\n",
    "nls97.set_index(\"personid\", inplace=True)\n",
    "\n",
    "feature_cols = ['satverbal','satmath','gpascience',\n",
    "  'gpaenglish','gpamath','gpaoverall']\n",
    "\n",
    "# 将 NLS 数据分为训练数据集和测试数据集\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(nls97[feature_cols],\\\n",
    "  nls97[['wageincome']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 删除与另一个特征高度相关的特征\n",
    "nls97.shape[0]\n",
    "X_train.info()\n",
    "y_train.info()\n",
    "X_test.info()\n",
    "y_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冗余特征\n",
    "\n",
    "redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_engine.selection as fesel\n",
    "pd.set_option('display.width', 80)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "nls97 = pd.read_csv(\"data/nls97b.csv\")\n",
    "nls97.set_index(\"personid\", inplace=True)\n",
    "ltpoland = pd.read_csv(\"data/ltpoland.csv\")\n",
    "ltpoland.set_index(\"station\", inplace=True)\n",
    "ltpoland.dropna()\n",
    "\n",
    "feature_cols = ['satverbal','satmath','gpascience',\n",
    "  'gpaenglish','gpamath','gpaoverall']\n",
    "\n",
    "# separate NLS data into train and test datasets\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(nls97[feature_cols],\\\n",
    "  nls97[['wageincome']], test_size=0.3, random_state=0)\n",
    "\n",
    "# remove a feature highly correlated with another\n",
    "X_train.corr()\n",
    "tr = fesel.DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.75)\n",
    "tr.fit(X_train)\n",
    "X_train_tr = tr.transform(X_train)\n",
    "X_test_tr = tr.transform(X_test)\n",
    "X_train_tr.info()\n",
    "\n",
    "feature_cols = ['year','month','latabs','latitude','elevation',\n",
    "  'longitude','country']\n",
    "\n",
    "# separate temperature data into train and test datasets\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(ltpoland[feature_cols],\\\n",
    "  ltpoland[['temperature']], test_size=0.3, random_state=0)\n",
    "X_train.sample(5, random_state=99)\n",
    "X_train.year.value_counts()\n",
    "X_train.country.value_counts()\n",
    "(X_train.latitude!=X_train.latabs).sum()\n",
    "\n",
    "# drop features with same values throughout dataset\n",
    "tr = fesel.DropConstantFeatures()\n",
    "tr.fit(X_train)\n",
    "X_train_tr = tr.transform(X_train)\n",
    "X_test_tr = tr.transform(X_test)\n",
    "X_train_tr.head()\n",
    "\n",
    "# drop features that have the same values as another feature\n",
    "tr = fesel.DropDuplicateFeatures()\n",
    "tr.fit(X_train_tr)\n",
    "X_train_tr = tr.transform(X_train_tr)\n",
    "X_train_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单热编码\n",
    "\n",
    "one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "pd.set_option('display.width', 80)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "nls97 = pd.read_csv(\"data/nls97b.csv\")\n",
    "nls97.set_index(\"personid\", inplace=True)\n",
    "\n",
    "\n",
    "feature_cols = ['gender','maritalstatus','colenroct99']\n",
    "nls97demo = nls97[['wageincome'] + feature_cols].dropna()\n",
    "\n",
    "# separate NLS data into train and test datasets\n",
    "X_demo_train, X_demo_test, y_demo_train, y_demo_test =  \\\n",
    "  train_test_split(nls97demo[feature_cols],\\\n",
    "  nls97demo[['wageincome']], test_size=0.3, random_state=0)\n",
    "\n",
    "# use get dummies to create dummies features\n",
    "pd.get_dummies(X_demo_train, columns=['gender','maritalstatus']).head(2).T\n",
    "pd.get_dummies(X_demo_train, columns=['gender','maritalstatus'],\n",
    "  drop_first=True).head(2).T\n",
    "\n",
    "# use the one hot encoder to create encoded features for gender and marital status\n",
    "ohe = OneHotEncoder(drop_last=True, variables=['gender','maritalstatus'])\n",
    "ohe.fit(X_demo_train)\n",
    "X_demo_train_ohe = ohe.transform(X_demo_train)\n",
    "X_demo_test_ohe = ohe.transform(X_demo_test)\n",
    "X_demo_train_ohe.filter(regex='gen|mar', axis=\"columns\").head(2).T\n",
    "\n",
    "\n",
    "# use the ordinal encoder for college enrollment\n",
    "X_demo_train.colenroct99.unique()\n",
    "X_demo_train.head()\n",
    "\n",
    "oe = OrdinalEncoder(categories=\\\n",
    "  [X_demo_train.colenroct99.unique()])\n",
    "colenr_enc = \\\n",
    "  pd.DataFrame(oe.fit_transform(X_demo_train[['colenroct99']]),\n",
    "    columns=['colenroct99'], index=X_demo_train.index)\n",
    "X_demo_train_enc = \\\n",
    "  X_demo_train[['gender','maritalstatus']].\\\n",
    "  join(colenr_enc)\n",
    "X_demo_train_enc.head()\n",
    "X_demo_train.colenroct99.value_counts().sort_index()\n",
    "X_demo_train_enc.colenroct99.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高卡度编码\n",
    "\n",
    "high cardinality encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import OneHotEncoder\n",
    "from category_encoders.hashing import HashingEncoder\n",
    "pd.set_option('display.width', 80)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "feature_cols = ['location','population',\n",
    "    'aged_65_older','region']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# Separate into train and test sets\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# use the one hot encoder for region\n",
    "X_train.region.value_counts()\n",
    "ohe = OneHotEncoder(top_categories=6, variables=['region'])\n",
    "covidtotals_ohe = ohe.fit_transform(covidtotals)\n",
    "covidtotals_ohe.filter(regex='location|region',\n",
    "  axis=\"columns\").sample(5, random_state=99).T\n",
    "\n",
    "# use the hashing encoder for region\n",
    "X_train['region2'] = X_train.region\n",
    "he = HashingEncoder(cols=['region'], n_components=6)\n",
    "X_train_enc = he.fit_transform(X_train)\n",
    "X_train_enc.\\\n",
    " groupby(['col_0','col_1','col_2','col_3','col_4',\n",
    "   'col_5','region2']).\\\n",
    " size().reset_index().rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征转换\n",
    "\n",
    "feature transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import transformation as vt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "\n",
    "feature_cols = ['location','population',\n",
    "    'aged_65_older','region']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# separate into train and test sets\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# show a histogram of total cases\n",
    "y_train.total_cases.skew()\n",
    "plt.hist(y_train.total_cases/1000000)\n",
    "plt.title(\"Total Covid Cases (in millions)\")\n",
    "plt.xlabel('Cases')\n",
    "plt.ylabel(\"Number of Countries\")\n",
    "plt.show()\n",
    "\n",
    "# do a log transformation on total cases\n",
    "tf = vt.LogTransformer(variables = ['total_cases'])\n",
    "y_train_tf = tf.fit_transform(y_train)\n",
    "\n",
    "y_train_tf.total_cases.skew()\n",
    "plt.hist(y_train_tf.total_cases)\n",
    "plt.title(\"Total Covid Cases (log transformation)\")\n",
    "plt.xlabel('Cases')\n",
    "plt.ylabel(\"Number of Countries\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# do a Box Cox transformation on total cases\n",
    "tf = vt.BoxCoxTransformer(variables = ['total_cases'])\n",
    "y_train_tf = tf.fit_transform(y_train)\n",
    "\n",
    "y_train_tf.total_cases.skew()\n",
    "plt.hist(y_train_tf.total_cases)\n",
    "plt.title(\"Total Covid Cases (Box Cox transformation)\")\n",
    "plt.xlabel('Cases')\n",
    "plt.ylabel(\"Number of Countries\")\n",
    "plt.show()\n",
    "\n",
    "stats.boxcox(y_train.total_cases)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征分选\n",
    "\n",
    "feature binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.discretisation import EqualFrequencyDiscretiser as efd\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser as ewd\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "\n",
    "feature_cols = ['location','population',\n",
    "    'aged_65_older','region']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# Separate into train and test sets\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# use qcut for bins\n",
    "y_train['total_cases_group'] = pd.qcut(y_train.total_cases, q=10, labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "y_train.total_cases_group.value_counts().sort_index()\n",
    "\n",
    "# set up function to run the transform\n",
    "def runtransform(bt, dftrain, dftest):\n",
    "  bt.fit(dftrain)\n",
    "  train_bins = bt.transform(dftrain)\n",
    "  test_bins = bt.transform(dftest)\n",
    "  return train_bins, test_bins\n",
    "\n",
    "# set up bins based on equal frequency\n",
    "y_train.drop(['total_cases_group'], axis=1, inplace=True)\n",
    "bintransformer = efd(q=10, variables=['total_cases'])\n",
    "y_train_bins, y_test_bins = runtransform(bintransformer, y_train, y_test)\n",
    "y_train_bins.total_cases.value_counts().sort_index()\n",
    "\n",
    "# set up bins based on equal width\n",
    "bintransformer = ewd(bins=10, variables=['total_cases'])\n",
    "y_train_bins, y_test_bins = runtransform(bintransformer, y_train, y_test)\n",
    "y_train_bins.total_cases.value_counts().sort_index()\n",
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "y_train_bins = y_train_bins.\\\n",
    "  rename(columns={'total_cases':'total_cases_group'}).\\\n",
    "  join(y_train)\n",
    "y_train_bins.groupby(\"total_cases_group\")[\"total_cases\"].agg(['min','max'])\n",
    "\n",
    "# use k means clustering\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')\n",
    "y_train_bins = \\\n",
    "  pd.DataFrame(kbins.fit_transform(y_train),\n",
    "  columns=['total_cases'])\n",
    "y_train_bins.total_cases.value_counts().sort_index()\n",
    "\n",
    "y_train.total_cases.agg(['skew','kurtosis'])\n",
    "y_train_bins.total_cases.agg(['skew','kurtosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征缩放\n",
    "\n",
    "feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "feature_cols = ['population','total_deaths',\n",
    "    'aged_65_older']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# separate into train and test sets\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# do min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_mms = pd.DataFrame(scaler.fit_transform(X_train),\n",
    "  columns=X_train.columns, index=X_train.index)\n",
    "X_train_mms.describe()\n",
    "\n",
    "\n",
    "# do standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_ss = \\\n",
    "  pd.DataFrame(scaler.fit_transform(X_train),\n",
    "  columns=X_train.columns, index=X_train.index)\n",
    "X_train_ss.describe()\n",
    "\n",
    "# use the robust scaler\n",
    "scaler = RobustScaler()\n",
    "X_train_rs = pd.DataFrame(scaler.fit_transform(X_train),\n",
    "  columns=X_train.columns, index=X_train.index)\n",
    "X_train_rs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 散列编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import OneHotEncoder\n",
    "from category_encoders.hashing import HashingEncoder\n",
    "\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "feature_cols = ['location','population',\n",
    "    'aged_65_older','region']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# 分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 使用单热编码器进行区域编码\n",
    "X_train.region.value_counts()\n",
    "ohe = OneHotEncoder(top_categories=6, variables=['region'])\n",
    "covidtotals_ohe = ohe.fit_transform(covidtotals)\n",
    "covidtotals_ohe.filter(regex='location|region',\n",
    "  axis=\"columns\").sample(5, random_state=99).T\n",
    "\n",
    "# 使用哈希编码器进行区域编码\n",
    "he = HashingEncoder(cols=['region'], n_components=16)\n",
    "covidtotals['region2'] = covidtotals.region\n",
    "covidtotals_enc = he.fit_transform(covidtotals)\n",
    "\n",
    "covidtotals_enc.filter(regex='col|reg', axis=\"columns\")\n",
    "covidtotals_enc.groupby(['col_0','col_1','col_2','col_3','col_4',\\\n",
    "    'col_5','col_6','col_7','col_8','col_9','col_10','col_11','col_12',\\\n",
    "        'col_13','col_14','col_15','region2']).size().reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
