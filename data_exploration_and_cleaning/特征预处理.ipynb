{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拆分数据-测试/训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.width', 75)\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "nls97 = pd.read_csv(\"data/nls97g.csv\", low_memory=False)\n",
    "nls97.set_index(\"personid\", inplace=True)\n",
    "\n",
    "feature_cols = ['satverbal','satmath','gpascience',\n",
    "  'gpaenglish','gpamath','gpaoverall']\n",
    "\n",
    "# 将NLS数据分离为训练和测试数据集\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(nls97[feature_cols],\\\n",
    "  nls97[['wageincome20']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 删除与另一个特征高度相关的特征\n",
    "nls97.shape[0]\n",
    "X_train.info()\n",
    "y_train.info()\n",
    "X_test.info()\n",
    "y_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冗余特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feature_engine.selection as fesel\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.width', 62)\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "nls97 = pd.read_csv(\"data/nls97g.csv\", low_memory=False)\n",
    "nls97.set_index(\"personid\", inplace=True)\n",
    "ltpoland = pd.read_csv(\"data/ltpoland.csv\")\n",
    "ltpoland.set_index(\"station\", inplace=True)\n",
    "\n",
    "ltpoland.dropna(inplace=True)\n",
    "\n",
    "feature_cols = ['satverbal','satmath','gpascience',\n",
    "  'gpaenglish','gpamath','gpaoverall']\n",
    "\n",
    "# 将NLS数据分离为训练和测试数据集\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(nls97[feature_cols],\\\n",
    "  nls97[['wageincome20']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 删除与另一个特征高度相关的特征\n",
    "X_train.corr()\n",
    "tr = fesel.DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.75)\n",
    "tr.fit(X_train)\n",
    "X_train_tr = tr.transform(X_train)\n",
    "X_test_tr = tr.transform(X_test)\n",
    "X_train_tr.info()\n",
    "\n",
    "feature_cols = ['year','month','latabs','latitude','elevation',\n",
    "  'longitude','country']\n",
    "\n",
    "# 将温度数据分离到训练和测试数据集中\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(ltpoland[feature_cols],\\\n",
    "  ltpoland[['temperature']], test_size=0.3, random_state=0)\n",
    "X_train.sample(5, random_state=99)\n",
    "X_train.year.value_counts()\n",
    "X_train.country.value_counts()\n",
    "(X_train.latitude!=X_train.latabs).sum()\n",
    "\n",
    "# 在整个数据集中删除具有相同值的特征\n",
    "tr = fesel.DropConstantFeatures()\n",
    "tr.fit(X_train)\n",
    "X_train_tr = tr.transform(X_train)\n",
    "X_test_tr = tr.transform(X_test)\n",
    "X_train_tr.head()\n",
    "\n",
    "# 删除与另一个特征具有相同值的特征\n",
    "tr = fesel.DropDuplicateFeatures()\n",
    "tr.fit(X_train_tr)\n",
    "X_train_tr = tr.transform(X_train_tr)\n",
    "X_train_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.width', 62)\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "nls97 = pd.read_csv(\"data/nls97g.csv\", low_memory=False)\n",
    "nls97.set_index(\"personid\", inplace=True)\n",
    "\n",
    "feature_cols = ['gender','maritalstatus','colenroct99']\n",
    "nls97demo = nls97[['wageincome20'] + feature_cols].dropna()\n",
    "\n",
    "# 将NLS数据分离为训练和测试数据集\n",
    "X_demo_train, X_demo_test, y_demo_train, y_demo_test =  \\\n",
    "  train_test_split(nls97demo[feature_cols],\\\n",
    "  nls97demo[['wageincome20']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 使用get dummies创建dummies功能\n",
    "pd.get_dummies(X_demo_train, \n",
    "  columns=['gender','maritalstatus'], dtype=float).\\\n",
    "  head(2).T\n",
    "pd.get_dummies(X_demo_train,\n",
    "  columns=['gender','maritalstatus'], dtype=float,\n",
    "  drop_first=True).head(2).T\n",
    "\n",
    "# 使用一键(one-hot)编码器为性别和婚姻状况创建编码特征\n",
    "ohe = OneHotEncoder(drop_last=True, variables=['gender','maritalstatus'])\n",
    "ohe.fit(X_demo_train)\n",
    "X_demo_train_ohe = ohe.transform(X_demo_train)\n",
    "X_demo_test_ohe = ohe.transform(X_demo_test)\n",
    "X_demo_train_ohe.filter(regex='gen|mar', axis=\"columns\").head(2).T\n",
    "\n",
    "\n",
    "# 在大学招生中使用序数(ordinal)编码器\n",
    "X_demo_train.colenroct99.\\\n",
    "  sort_values().unique()\n",
    "X_demo_train.head()\n",
    "X_demo_train.info()\n",
    "\n",
    "oe = OrdinalEncoder(categories=\\\n",
    "  [X_demo_train.colenroct99.sort_values().\\\n",
    "   unique()])\n",
    "colenr_enc = \\\n",
    "  pd.DataFrame(oe.fit_transform(X_demo_train[['colenroct99']]),\n",
    "    columns=['colenroct99'], index=X_demo_train.index)\n",
    "X_demo_train_enc = \\\n",
    "  X_demo_train[['gender','maritalstatus']].\\\n",
    "  join(colenr_enc)\n",
    "X_demo_train_enc.head()\n",
    "X_demo_train.colenroct99.value_counts().\\\n",
    "  sort_index()\n",
    "X_demo_train_enc.colenroct99.value_counts().\\\n",
    "  sort_index()\n",
    "  \n",
    "X_demo_train.head()\n",
    "\n",
    "nls97.loc[764231,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高可靠性编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from category_encoders.hashing import HashingEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.width', 72)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "feature_cols = ['location','population',\n",
    "    'aged_65_older','life_expectancy','region']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# 分为训练组和测试组\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 对区域使用一个热编码器\n",
    "X_train.region.value_counts()\n",
    "ohe = OneHotEncoder(top_categories=6, variables=['region'])\n",
    "covidtotals_ohe = ohe.fit_transform(covidtotals)\n",
    "covidtotals_ohe.filter(regex='location|region',\n",
    "  axis=\"columns\").sample(5, random_state=2).T\n",
    "\n",
    "# 对区域使用哈希编码器\n",
    "X_train['region2'] = X_train.region\n",
    "he = HashingEncoder(cols=['region'], n_components=6)\n",
    "X_train_enc = he.fit_transform(X_train)\n",
    "X_train_enc.\\\n",
    " groupby(['col_0','col_1','col_2','col_3','col_4',\n",
    "   'col_5','region2']).\\\n",
    " size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from feature_engine import transformation as vt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "\n",
    "feature_cols = ['location','population',\n",
    "    'aged_65_older','life_expectancy','region']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# 分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 显示总病例的直方图\n",
    "y_train.total_cases.skew()\n",
    "plt.hist(y_train.total_cases/1000000)\n",
    "plt.title(\"Total Covid Cases (in millions)\")\n",
    "plt.xlabel('Cases')\n",
    "plt.ylabel(\"Number of Countries\")\n",
    "plt.show()\n",
    "\n",
    "# 对总案例进行日志转换\n",
    "tf = vt.LogTransformer(variables = ['total_cases'])\n",
    "y_train_tf = tf.fit_transform(y_train)\n",
    "\n",
    "y_train_tf.total_cases.skew()\n",
    "plt.hist(y_train_tf.total_cases)\n",
    "plt.title(\"Total Covid Cases (log transformation)\")\n",
    "plt.xlabel('Cases')\n",
    "plt.ylabel(\"Number of Countries\")\n",
    "plt.show()\n",
    "\n",
    "# 对所有案例进行Box-Cox变换\n",
    "tf = vt.BoxCoxTransformer(variables = ['total_cases'])\n",
    "y_train_tf = tf.fit_transform(y_train)\n",
    "\n",
    "y_train_tf.total_cases.skew()\n",
    "\n",
    "plt.hist(y_train_tf.total_cases)\n",
    "plt.title(\"Total Covid Cases (Box Cox transformation)\")\n",
    "plt.xlabel('Cases')\n",
    "plt.ylabel(\"Number of Countries\")\n",
    "plt.show()\n",
    "\n",
    "stats.boxcox(y_train.total_cases)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征分选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from feature_engine.discretisation import EqualFrequencyDiscretiser as efd\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser as ewd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "\n",
    "feature_cols = ['location','population',\n",
    "    'aged_65_older','life_expectancy','region']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# 分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 使用 qcut 进行分仓\n",
    "y_train['total_cases_group'] = \\\n",
    "  pd.qcut(y_train.total_cases, q=10, \n",
    "  labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "y_train.total_cases_group.value_counts().\\\n",
    "  sort_index()\n",
    "\n",
    "# 设置运行转换的函数\n",
    "def runtransform(bt, dftrain, dftest):\n",
    "  bt.fit(dftrain)\n",
    "  train_bins = bt.transform(dftrain)\n",
    "  test_bins = bt.transform(dftest)\n",
    "  return train_bins, test_bins\n",
    "\n",
    "# 根据等频设置分区\n",
    "y_train.drop(['total_cases_group'], axis=1, inplace=True)\n",
    "bintransformer = efd(q=10, variables=['total_cases'])\n",
    "y_train_bins, y_test_bins = runtransform(bintransformer, y_train, y_test)\n",
    "y_train_bins.total_cases.value_counts().sort_index()\n",
    "\n",
    "# 以等宽为基础设仓\n",
    "bintransformer = ewd(bins=10, variables=['total_cases'])\n",
    "y_train_bins, y_test_bins = runtransform(bintransformer, y_train, y_test)\n",
    "y_train_bins.total_cases.value_counts().sort_index()\n",
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "y_train_bins = y_train_bins.\\\n",
    "  rename(columns={'total_cases':'total_cases_group'}).\\\n",
    "  join(y_train)\n",
    "y_train_bins.groupby(\"total_cases_group\")[\"total_cases\"].\\\n",
    "  agg(['min','max'])\n",
    "\n",
    "# 使用 k 手段聚类\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal',\n",
    "  strategy='kmeans', subsample=None, random_state=123)\n",
    "y_train_bins = \\\n",
    "  pd.DataFrame(kbins.fit_transform(y_train),\n",
    "  columns=['total_cases'], index=y_train.index)\n",
    "y_train_bins.total_cases.value_counts().sort_index()\n",
    "\n",
    "\n",
    "y_train.total_cases.agg(['skew','kurtosis'])\n",
    "y_train_bins.total_cases.agg(['skew','kurtosis'])\n",
    "\n",
    "y_train_bins.rename(columns={'total_cases':'total_cases_bin'}, inplace=True)\n",
    "y_train.join(y_train_bins).\\\n",
    "  groupby(['total_cases_bin'])['total_cases'].\\\n",
    "  agg(['min','max','size'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "pd.set_option('display.width', 69)\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "\n",
    "feature_cols = ['population','total_deaths',\n",
    "    'aged_65_older','life_expectancy']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# 分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 进行最小最大缩放\n",
    "scaler = MinMaxScaler()\n",
    "X_train_mms = pd.DataFrame(scaler.fit_transform(X_train),\n",
    "  columns=X_train.columns, index=X_train.index)\n",
    "X_train_mms.describe()\n",
    "\n",
    "# 进行标准缩放\n",
    "scaler = StandardScaler()\n",
    "X_train_ss = \\\n",
    "  pd.DataFrame(scaler.fit_transform(X_train),\n",
    "  columns=X_train.columns, index=X_train.index)\n",
    "X_train_ss.describe()\n",
    "\n",
    "# 使用稳健的定标器\n",
    "scaler = RobustScaler()\n",
    "X_train_rs = pd.DataFrame(scaler.fit_transform(X_train),\n",
    "  columns=X_train.columns, index=X_train.index)\n",
    "X_train_rs.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 哈希编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from category_encoders.hashing import HashingEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "h = FeatureHasher(n_features=8, input_type='string', alternate_sign=False)\n",
    "pd.set_option('display.width', 80)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "feature_cols = ['location','population',\n",
    "    'aged_65_older','diabetes_prevalence','region']\n",
    "# KeyError: \"['diabetes_prevalence'] not in index\"\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# 分为训练组和测试组\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "oe = OrdinalEncoder(encoding_method='arbitrary', \n",
    "  variables=['region'])\n",
    "\n",
    "X_train = oe.fit_transform(X_train)\n",
    "X_train.region.value_counts().sort_index()\n",
    "\n",
    "X_train.region\n",
    "\n",
    "# TypeError: 'int' object is not iterable\n",
    "f = h.transform(X_train.region)\n",
    "f.toarray()\n",
    "temp = pd.DataFrame(f.toarray(), index=X_train.index) \n",
    "type(temp)\n",
    "temp.shape\n",
    "temp\n",
    "X_train\n",
    "\n",
    "test = pd.DataFrame({'type': ['a', 'b', 'a', 'c', 'b']})\n",
    "test\n",
    "f = h.transform(test.type)\n",
    "f.toarray()\n",
    "\n",
    "# 对区域使用一个热编码器\n",
    "X_train.region.value_counts()\n",
    "ohe = OneHotEncoder(top_categories=6, variables=['region'])\n",
    "covidtotals_ohe = ohe.fit_transform(covidtotals)\n",
    "covidtotals_ohe.filter(regex='location|region',\n",
    "  axis=\"columns\").sample(5, random_state=99).T\n",
    "\n",
    "# 对区域使用哈希编码器\n",
    "he = HashingEncoder(cols=['region'], n_components=6)\n",
    "covidtotals_enc = he.fit_transform(covidtotals)\n",
    "covidtotals_enc = covidtotals_enc.join(covidtotals[['region']])\n",
    "covidtotals_enc[['col_0','col_1','col_2','col_3',\n",
    "  'col_4','col_5','region']].sample(5, random_state=1)\n",
    "\n",
    "data = pd.DataFrame([\n",
    "    ['value_1', 23],\n",
    "    ['value_2', 13],\n",
    "    ['value_3', 42],\n",
    "    ['value_4', 13],\n",
    "    ['value_2', 46],\n",
    "    ['value_1', 28],\n",
    "    ['value_2', 32],\n",
    "    ['value_3', 87],\n",
    "    ['value_4', 98],\n",
    "    ['value_5', 86],\n",
    "    ['value_3', 45],\n",
    "    ['value_2', 73],\n",
    "    ['value_1', 36],\n",
    "    ['value_3', 93]\n",
    "], columns = ['feature1', 'feature2'])\n",
    "\n",
    "data['feature1b'] = data.feature1\n",
    "\n",
    "feature_hasher = FeatureHasher(n_features = 3, input_type = 'string')\n",
    "\n",
    "temp2 = pd.DataFrame(feature_hasher.fit_transform(data['feature1']).toarray())\n",
    "temp2\n",
    "\n",
    "pd.concat([\n",
    "pd.DataFrame(feature_hasher.fit_transform(data['feature1']).toarray()),\n",
    "data[['feature1b','feature2']]], axis = 1)\n",
    "\n",
    "h.fit_transform(data[['country']].to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from category_encoders.hashing import HashingEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "covidtotals = pd.read_csv(\"data/covidtotals.csv\")\n",
    "feature_cols = ['location','population',\n",
    "    'aged_65_older','diabetes_prevalence','region']\n",
    "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
    "\n",
    "# 分为训练组和测试组\n",
    "X_train, X_test, y_train, y_test =  \\\n",
    "  train_test_split(covidtotals[feature_cols],\\\n",
    "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
    "\n",
    "# 对区域使用一个热编码器\n",
    "X_train.region.value_counts()\n",
    "ohe = OneHotEncoder(top_categories=6, variables=['region'])\n",
    "covidtotals_ohe = ohe.fit_transform(covidtotals)\n",
    "covidtotals_ohe.filter(regex='location|region',\n",
    "  axis=\"columns\").sample(5, random_state=99).T\n",
    "\n",
    "# 对区域使用哈希编码器\n",
    "he = HashingEncoder(cols=['region'], n_components=16)\n",
    "covidtotals['region2'] = covidtotals.region\n",
    "covidtotals_enc = he.fit_transform(covidtotals)\n",
    "\n",
    "covidtotals_enc.filter(regex='col|reg', axis=\"columns\")\n",
    "covidtotals_enc.groupby(['col_0','col_1','col_2','col_3','col_4','col_5','col_6','col_7','col_8','col_9','col_10','col_11','col_12','col_13','col_14','col_15','region2']).size().reset_index()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
